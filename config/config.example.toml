# =============================================
#         LLM Configuration Settings
# =============================================

[llm]
# Main LLM configuration (Anthropic Claude by default)
model = "claude-3-7-sonnet-20250219"       # Model identifier
base_url = "https://api.anthropic.com/v1/" # API endpoint
api_key = "YOUR_API_KEY"                   # Your secret API key
max_tokens = 8192                          # Response length limit (1-8192)
temperature = 0.0                          # Creativity control (0=precise, 1=creative)

# Alternative provider examples (commented out):

# ----------------------------
# Amazon Bedrock Configuration
# ----------------------------
# [llm]
# api_type = "aws"  # Required for Bedrock
# model = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"
# base_url = "bedrock-runtime.us-west-2.amazonaws.com"
# max_tokens = 8192
# temperature = 1.0
# api_key = "bear"  # Required placeholder

# --------------------------
# Azure OpenAI Configuration
# --------------------------
# [llm]
# api_type = "azure"
# model = "YOUR_MODEL_NAME"  # e.g., "gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPLOYMENT_ID}"
# api_key = "YOUR_AZURE_API_KEY"
# max_tokens = 8096
# temperature = 0.0
# api_version = "2024-08-01-preview"

# --------------------
# Ollama Configuration
# --------------------
# [llm]
# api_type = "ollama"
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"  # Can be any string for local Ollama
# max_tokens = 4096
# temperature = 0.0


# =============================================
#        Vision Model Configuration
# =============================================

[llm.vision]
# Vision-specific model settings
model = "claude-3-7-sonnet-20250219"       # Vision-capable model
base_url = "https://api.anthropic.com/v1/" # API endpoint
api_key = "YOUR_API_KEY"                   # API key (can be same as main LLM)
max_tokens = 8192                          # Response length limit
temperature = 0.0                          # Creativity control

# Ollama Vision Example:
# [llm.vision]
# api_type = "ollama"
# model = "llama3.2-vision"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0


# =============================================
#         Browser Configuration
# =============================================

[browser]
headless = false         # Run browser without GUI (true/false)
disable_security = true  # Disable security restrictions (careful!)
extra_chromium_args = [] # Additional Chrome/Chromium flags
# chrome_instance_path = "" # Path to existing Chrome binary
# wss_url = ""             # Connect via WebSocket
# cdp_url = ""             # Connect via Chrome DevTools Protocol

# --------------------------
# Proxy Settings (Optional)
# --------------------------
[browser.proxy]
# server = "http://proxy-server:port"  # Proxy address
# username = "proxy-username"         # Auth credentials
# password = "proxy-password"         # Auth credentials


# =============================================
#         Search Engine Configuration
# =============================================

[search]
engine = "Google"                          # Primary search engine
fallback_engines = ["DuckDuckGo", "Baidu"] # Fallback order
retry_delay = 60                           # Seconds between retries
max_retries = 3                            # Maximum retry attempts


# =============================================
#         Sandbox Configuration
# =============================================

[sandbox]
use_sandbox = false        # Enable code execution sandbox
image = "python:3.12-slim" # Docker image for sandbox
work_dir = "/workspace"    # Working directory in container
memory_limit = "1g"        # RAM limit (e.g., 512m, 1g)
cpu_limit = 2.0            # CPU cores allocation
timeout = 300              # Execution timeout in seconds
network_enabled = true     # Allow network access
