# Global LLM configuration
# 全局大语言模型配置
[llm]
model = "gemini-2.0-flash"       # 使用的LLM模型（英文注释保留）
base_url = "https://generativelanguage.googleapis.com/v1beta/openai/"                              # API端点URL
api_key = ""                   # 您的API密钥
max_tokens = 8192                          # 响应中允许的最大token数
temperature = 0.0                          # 控制生成随机性（0-1，0最确定）

# [llm] # Amazon Bedrock 配置示例
# api_type = "aws"                                       # 必须设置为aws
# model = "us.anthropic.claude-3-7-sonnet-20250219-v1:0" # Bedrock支持的模型ID
# base_url = "bedrock-runtime.us-west-2.amazonaws.com"   # 当前未使用
# max_tokens = 8192                                      # 最大token数
# temperature = 1.0                                      # 生成随机性（1.0最随机）
# api_key = "bear"                                       # 必须填写但实际不使用

# [llm] #Azure OpenAI 配置示例:
# api_type= 'azure'                              # 必须设置为azure
# model = "YOUR_MODEL_NAME"                     # 模型名称 例如："gpt-4o-mini"
# base_url = "{YOUR_AZURE_ENDPOINT.rstrip('/')}/openai/deployments/{AZURE_DEPOLYMENT_ID}"  # Azure端点URL
# api_key = "AZURE API KEY"                     # Azure API密钥
# max_tokens = 8096                             # 最大token数限制
# temperature = 0.0                             # 生成确定性控制
# api_version="AZURE API VERSION"               # API版本 例如："2024-08-01-preview"

# [llm] #OLLAMA:
# api_type = 'ollama'
# model = "llama3.2"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# Optional configuration for specific LLM models
# 特定LLM模型的视觉配置
[llm.vision]
model = "gemini-2.0-flash"       # 使用的视觉模型
base_url = "https://generativelanguage.googleapis.com/v1beta/openai/" # 视觉模型API端点URL
api_key = ""                   # 视觉模型API密钥
max_tokens = 8192                          # 响应最大token数限制
temperature = 0.0                          # 生成随机性控制（0最确定）

# [llm.vision] #OLLAMA VISION:
# api_type = 'ollama'
# model = "llama3.2-vision"
# base_url = "http://localhost:11434/v1"
# api_key = "ollama"
# max_tokens = 4096
# temperature = 0.0

# Optional configuration for specific browser configuration
# [browser]
# Whether to run browser in headless mode (default: false)
#headless = false
# Disable browser security features (default: true)
#disable_security = true
# Extra arguments to pass to the browser
#extra_chromium_args = []
# Path to a Chrome instance to use to connect to your normal browser
# e.g. '/Applications/Google Chrome.app/Contents/MacOS/Google Chrome'
#chrome_instance_path = ""
# Connect to a browser instance via WebSocket
#wss_url = ""
# Connect to a browser instance via CDP
#cdp_url = ""

# Optional configuration, Proxy settings for the browser
# [browser.proxy]
# server = "http://proxy-server:port"
# username = "proxy-username"
# password = "proxy-password"

# Optional configuration, Search settings.
# [search]
# Search engine for agent to use. Default is "Google", can be set to "Baidu" or "DuckDuckGo".
#engine = "Google"
# Fallback engine order. Default is ["DuckDuckGo", "Baidu"] - will try in this order after primary engine fails.
#fallback_engines = ["DuckDuckGo", "Baidu"]
# Seconds to wait before retrying all engines again when they all fail due to rate limits. Default is 60.
#retry_delay = 60
# Maximum number of times to retry all engines when all fail. Default is 3.
#max_retries = 3


## Sandbox configuration
#[sandbox]
#use_sandbox = false
#image = "python:3.12-slim"
#work_dir = "/workspace"
#memory_limit = "1g"  # 512m
#cpu_limit = 2.0
#timeout = 300
#network_enabled = true

# MCP (Model Context Protocol) configuration
[mcp]
server_reference = "app.mcp.server" # default server module reference
