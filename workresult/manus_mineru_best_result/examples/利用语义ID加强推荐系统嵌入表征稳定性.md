
> 论文：《Enhancing Embedding Representation Stability in Recommendation Systems with Semantic ID》
> 
> 发布时间：2025年4月2日（热乎）
> 
> 推荐理由：
> 
> 1.  “强烈推荐看看”论文；
> 
> 2.  嵌入表征是推荐系统信息聚合层的基石，如果方法是solid的话，那对于我们的各项模型优化，策略迭代都具有重大的指导意义；
> 
> 3.  文章聚焦于利用语义id，来优化推荐系统当中的海量的嵌入表征问题，且是在meta推荐场景有工业实验验证。我们的场景又存在的大量的可以转化为语义ID的物品ID，包括但不限于用户ID，广告ID，文章ID，素材ID等，去年流量主特征修复的pltv模型放量，也是对流量主ID进行修复的，这些种种都说明推荐系统当中的语义ID的重要性；

# 一、  背景

## 1.1 推荐系统当中的物品ID的嵌入建模现状

推荐系统当中物品ID的特点：

● 物品ID基数特别大：用户ID，广告ID，文章ID，流量主ID，素材ID，很轻松的就上百万乃至上亿，甚至是上十亿的级别；

● 少量的物品ID贡献绝大多数的用户点击和转化：高活用户的行为尤其丰富，爆款的游戏，爆量的广告，爆款的文章和视频，爆单的直播间，这些丰富行为的背后，都让部分少量的ID聚集积累了海量的行为数据；

● ID的“漂移”：大量的ID快速的进来，又有大量的ID很快就离开：新增用户和流失用户，都在快速的变化；

目前针对物品id嵌入表征，非常简单实用的方法就是：随机hash之后再通过嵌入词表来学习嵌入（embedding）；

然而，随机hash算法存在不少的问题：

1、  物品ID量级巨大，hash碰撞导致梯度学习紊乱；

项目基数庞大和对嵌入表大小的系统限制，使用随机哈希简单实用。然而，由于随机哈希碰撞的性质，这导致对嵌入权重的梯度更新矛盾。

2、  不同物品累积的用户行为交互数据，差异巨大，中长尾物品学习差；

中长尾的物品ID积累的用户行为很少，学习的效果很差。

3、  新物品的冷启学习效果差；

随着系统中的项目随时间从ID漂移变化，旧项目的学习丢失，新项目的嵌入权重学习，本质上几乎是随机的，学习效果差。

## 1.2 嵌入表征的一些迭代演进

### 广告系统3.0演进

[https://km.woa.com/articles/show/608358?kmref=search&from_page=1&no=1](https://km.woa.com/articles/show/608358?kmref=search&from_page=1&no=1)

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_538576_Dw6LqaFCj5bbVVeW_1744084686?w=2136&h=1156&type=image/png)

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_555420_HLM8iMOQJNTGefgH_1744084699?w=2120&h=1126&type=image/png)

### 大模型的embedding嵌入

[大模型embedding服务](https://doc.weixin.qq.com/doc/w3_AKQAQQZcACoc0KgAZUbRwK092qFxy?scode=AJEAIQdfAAopiaH16OAKQAQQZcACo)

|   |   |   |
|---|---|---|
|特点|SimCSE|davinci-001|
|编码器|Bert|GPT-3|
|embedding输出|利用cls标记位输出|利用特殊eos的token的嵌入当做输出，能够拿到前面所有词的表征|
|无监督阶段|利用dropout来给相同的正例以不同的噪声；<br><br>利用同batch的其他case，当做负例；<br><br>![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_241505_wzuLHSUfy_ysQ964_1717641966?w=623&h=276&type=image/png)|利用eos特殊的token的不同引入正例之间的差异；<br><br>利用同batch的其他case，当做负例；<br><br>![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_376985_cL1StgjhZPS0dFQ8_1717642084?w=730&h=589&type=image/png)|
|有监督阶段|利用之前提过的文本蕴含类问题为例，以entailment当做正例，以contradiction为复例，参考附录7；|同SimCSE|
|损失函数|正样本尽可能靠近，负样本尽可能远离；<br><br>![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_504299_1eZdRv1EL1QHou_z_1717642283?w=772&h=167&type=image/png)|将同一个batch里面的正样本和负样本的交叉熵求和；<br><br>![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_146012_WquTeht1z17I7GjD_1717642355?w=718&h=502&type=image/png)|

### 其他的嵌入表征的解决方案

Google，23年除了发布TIGER，利用RQ-VAE生成语义ID进行生成式召回以外，还发布了一篇论文，利用RQ-VAE生成语义ID来进行推荐系统的嵌入表征优化。不过这篇论文当时标题是一个case研究而已，meta这篇论文我认为跟23年google的思路是一致的，不过meta直接说我已经在旗舰的广告系统上，运行1年了，获得了0.15%的归一化熵的显著提升了，很有意思。

## 1.3 生成式推荐

前序的生成式推荐，无论是TIGER还是OneREC这两篇论文，都用到了语义ID编码的应用：将推荐的item，转为语义ID序列，然后进行生成式的召回和排序。

TIGER：生成式召回的推荐系统[【大模型调研】推荐系统中的生成式 LLMs应用专题调研](https://doc.weixin.qq.com/doc/w3_AagAMQb2ACw0JCFrwZ1TI21iukCq1?scode=AJEAIQdfAAos6XSgQxAKQAQQZcACo)

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_961503_gpwocZBkkGZBoUu5_1744179629?w=1806&h=746&type=image/png)

OneREC：召回排序大一统的生成式推荐系统[【推荐技术分享】快手OneRec端到端生成式推荐系统](https://doc.weixin.qq.com/doc/w3_AagAMQb2ACwv7DtDnp0QwyPqd11Dd?scode=AJEAIQdfAAoEsbRGfn)

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_322763_SJvFzVgWTAjGzVRR_1744179739?w=2068&h=1302&type=image/png)

## 1.4 AI助手加密通话

两个AI利用他们生成的token（Gibbellink模式）进行相互交流，减少了很多中间token转化的方式，提升了沟通效率？！？

[南方都市报](https://m.mp.oeeee.com/a/BAAFRD0000202502261054965.html)

是否对于AI来说，我们的语言，每个物品都是组合后单独命名的思维方式，有些过度定制化和效率低下了。

比如我们说苹果和葡萄的异同点，可以用语言说出来很多：

1.  他们都是水果，可以吃的；

2.  他们都是圆的；

3.  一个是甜脆口味，一个是酸甜糯糯的口味；

...

对于AI来说，是否直接通过语义ID来理解更快？

苹果：[5，22，78]；(水果，甜，圆)

葡萄：[5，20，78]；(水果，酸甜，圆)

# 二、核心贡献

本文的主要贡献包括：

● 通过对Meta生产广告排名模型的简化版本进行实验，加深了对语义ID如何提高嵌入表示稳定性的实证理解。论文提出了语义ID的prefix-ngram参数化，这是一种在语义ID之上的新颖标记参数化技术，与google的那篇论文引入的原始语义ID的SPM的方式相比，带来了显著的性能提升（并没有提到）。

● 论文从项目数量（项目基数）、大多数项目曝光次数较少（曝光偏差）以及系统中项目生命周期短（ID漂移）等方面描述了项目数据分布，并解释了它们与嵌入表示稳定性的联系。

● 论文描述了将语义ID的prefix-ngram转化为Meta生产系统中的稀疏特征和顺序特征的详细过程。展示了添加这些特征带来的在线性能提升，并提高了在线预测稳定性。

# 三、  详细方案说明

## 3.1 整体模型场景

深度学习下的推荐系统模型，大致的模型结构都是由三个堆叠的部分组成。

第一部分是信息聚合部分，在这里稀疏（即分类）、密集以及基于用户历史的特征被独立处理。这些模块的每一个的输出都是一系列嵌入向量。

其次，这些被连接成一个单一列表，该列表通过交互层，在此对所有向量对进行点积（或更高阶的交互）。

第三，交互层的输出通过一个多层感知器（MLP）转换，以产生逻辑分数，并采用sigmoid函数输出一个排序概率。

该模型使用交叉熵损失进行训练。

[https://doc.weixin.qq.com/flowchart-addon](https://doc.weixin.qq.com/flowchart-addon)

这篇论文聚焦的场景是在信息聚合层，传统的方案是将很多文本类的特征，采取随机hash的方法，将很多广告id，用户的行为id序列等，通过随机hash的方法，生成hashid，然后通过hashid（取模），再去embedding table里面，找到对应的embedding，再利用这个embedding来进行训练迭代。

该论文提出的方法，是利用各种内容理解模型（文本生成理解模型text->embedding，图片生成理解模型image->embedding等其他多模态的模型），先训练RA-VAE，产生语义ID，然后基于这个语义ID，再转化到embedding的table当中的行序列（码本token参数化），进行looking up，找到对应的嵌入表征，基于这个嵌入表征再去训练推荐系统的排序模型。

因此评估的思路是：

基线：随机hash训练完之后的模型交叉熵（归一化，跟真实正负样本的分布熵做归一化）的归一化结果

实验组：RQ-VAE+码本token参数化之后的模型的交叉熵（同样归一化）的归一化熵；

当然模型的归一化熵如果越小，说明这个模型拟合到的正负样本的分布更好，也就说明这个模型的嵌入表征的方法更优。

ps，归一化交叉熵的评估指标：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_565328_e9oF6ipsmsoRtnwl_1744186003?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

## 3.2 语义ID的生成

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_433891_HLXgSZ-aUTO2PV6s_1744179426?w=878&h=498&type=image/png)

语义ID的生成，是分两个阶段学习的：

首先，应用内容理解模型到物品的文本、图像或视频上，生成密集的内容嵌入。

然后，在内容嵌入上训练一个RQ-VAE，以获得每个项目的向量量化，该量化表示为一系列从粗略到精细的离散代码，称为项目的语义ID。

设L为层数（即序列的长度），K为码本大小（即每个层次的聚类数量）；

RQ-VAE由一个编码器、一个残差量化器（逐级用码本量化）和一个解码器组成。

编码器将内容嵌入x映射到一个连续的潜在表示z，残差量化器将z量化成一系列离散码c:=(c1 ,…,cL )，解码器从c重构x。离散码序列c:=(c1 ,…,cL )是分层的：cl 对应于第l层的码本当中那个v_c，这个vector近似于rl ，即从第(l−1)层到第1层递归应用码本向量后z的剩余残差。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_666562_C7Ui9y4-Kw8OuX-z_1744183460?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

拟合了这个残差之后，最后解码器再根据这个码本向量（(c1 ,…,cL )）重新解构成原始的input的x，经过不断地迭代，就可以训练好这个RQ-VAE模型。

整个过程，RQ-VAE的损失函数由两个方面组成，分别是编码后重构的解码差异的loss，还有就是每一层残差拟合的相近程度的loss：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_233558_uV8p94ka4f8Ilmkp_1744184339?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

问题：将物品ID转化为语义ID，有哪些好处？

语义ID，其实是相当于无监督的对全量的物品ID进行了层次聚类，聚类之后，其实我们对原始的物品ID之间的关联关系，就有了更深的认识，而且是一个递进的认识；

比如，苹果，葡萄，其实就是两个物品ID，如果只是随机hash过程，这两个物品ID的结果没有任何关系，但是转化为语义ID之后，他们可能至少有1-2个前缀的码本ID是一致的。

苹果：[5(水果)，22(甜)，78(圆)]

葡萄：[5(水果)，20(酸甜)，78(圆)]

## 3.3 语义ID的参数化

前面的步骤，就像TIGER和OneREC一样，很多离散的物品（交互过的物品id）都转化为了码本token序列，编程了语义ID。但是这个语义ID本身只是基于物品的一些基础描述信息生成的，基本可以理解是无监督产生的。

这个语义ID当然本身可以用于传统的推荐系统，当时如何跟现有的推荐系统的信息聚合层的embedding looking up相结合。不像现在大火的生成式推荐模型，可以完全基于语义ID进行召回排序推荐，传统的推荐模型需要将物品id转化为嵌入embedding，然后再通过特征交叉和mlp进行梯度迭代来学习。

因此本文再有了码本token序列之后，需要将码本token序列转化为可以学习的embedding，然后继续跟着推荐系统的其他部分一起学习，这样完全兼容之前的推荐系统范式，同时原有的物品ID转化为语义ID也更有意义。

因此，需要有一个f(x)，将语义ID，转化为学习的embedding，尽可能要利用到前序语义ID已经具备的层次化理解的结果，又要避免随机hash方式的随机无意义的序号碰撞

问题：为什么不采用码本token序列对应的embedding，直接当做排序模型的embedding输入去训练？

码本序列的每一个码本确实是对应了一个embedding，但是这个embedding的学习过程是仅仅跟内容理解模型有关的，相当于仅仅是从物品认知层对物品的嵌入表征。

这个嵌入表征无关推荐系统当中的行为影响，比如是否有用户点击，以及用户点击反馈的情况的影响。

而推荐系统的训练当中需要进一步训练的embedding，是需要将固定的物品ID，或者说语义ID，跟用户的交互情况去联合建模排序的，是一个跟用户主观行为相关会变化的可学习的嵌入表征，而不是客观的语义ID。

本文采取n-gram模型，将语义ID的码本序列，转为embedding looking up的表行序列：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_278939_ABBwuXbKEjjFx2g4_1744190442?w=858&h=252&type=image/png)

可以看到语义ID的第一个码本序列c_1，不管什么n-gram模型，他的权重系数都很大，也体现了语义ID分层逐级来训练的过程，就像我们认识物品，是先从分类开始，在逐渐加深“当然一级分类，二级分类，三级分类的重要性逐渐降低”。

有了这个码本token参数化之后，再跟着推荐系统其他模块继续训练，可以发现不同的n-gram的模型还是存在效果差异：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_360989_VLAhrz_UyKla7rCz_1744190801?w=872&h=448&type=image/png)

核心结论：

i) prefix-ngram 是最佳的参数化方式。这表明在嵌入表映射中结合聚类的层次特性是必要的，因为它允许在比平面映射更多的项目之间共享知识；

ii) 增加prefix-ngram 的深度可以提高神经网络嵌入（NE）性能；

iii) 增加查询变分自编码器（RQ-VAE）的基数可以提高NE性能，语义ID更细腻；

问题：为什么考虑n-gram模型，进一步为什么prefix-ngram方式是合适的？

1.  语义ID通常比较短，就3-6个字符而已，而且也足够表示相当大范围的物品ID，论文K = 2048，三层的RQ-VAE，就可以表征十亿级；论文当中生产环境，更是高达6层；

n-gram模型就是最传统的本文表示方法，只是随着语言序列的不断加长，逐步从n-gram到RNN，再到Transformer模型演变，但是本文场景使用n-gram已经足够了；

2.  prefix-ngram，码本序列的不同的码本组合方式都考虑到的，而且分层递进的考虑前面的码本的影响（前面的码本当然是最重要的），如下图

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_447315_Iv6QwAzJvrYRqfOE_1744190189?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

其他的一些表征方法我认为也是可以的，比如《Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations.》当中提到的google的Sentence Piece Models（SPM）方法，通过切分不同码本的切词组合方式来考虑embedding looking up的表行序号。

但是可惜本论文虽然说更好，但是并没有对比？不知道为啥，存疑。

# 四、  离线验证

## 4.1 对比基线和参数设置

两个对比的baseline：

1.  唯一嵌入

每个原始ID都有自己的嵌入表行。虽然由于系统限制在生产场景中不现实，但论文考虑这个模型是为了说明目的。在评估期间，训练期间未见过的ID将被映射到一个随机初始化的未训练嵌入。

2.  随机哈希

当物品全量基数 I≈a⋅H ，对于某些 a>1 时，我们可以将原始ID随机哈希到嵌入表行，当时会产生一定的hash碰撞，也就是部分的物品ID会使用相同的嵌入表征。

参数设置：

语义ID 物品的内容嵌入是从一个多模态图像和文本基础模型获得的。该基础模型使用图像和文本对齐目标在大量训练物品集上进行预训练。

使用meta在过去三个月内所有目标物品的内容嵌入上训练RQ-VAE，其中 L=3 且 K=2048，物品ID的总量超过1亿。

## 4.2 语义表征分析

### 头部、中长尾行为的物品嵌入效果分析

分析背景：不同的物品ID的用户行为数量分布情况：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_143064_Ukr-kaBSRISLAlAV_1744192950?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

前0.1%的“头部”项目占据了全部项目印象（用户行为）的25%，接下来的5.5%的“躯干”项目占据了累计印象的50%，而其余94.4%的“尾部”项目占据了剩余的25%印象。

根据如上的项目划分，论文将语义ID在此排序顺序中产生的累积展示次数的百分比分为头部、躯干和尾部项目，分别是25%、75%或100%。由于展示偏斜，头部、躯干或尾部项目的百分比分别为0.1%、5.5%或94.4%（如上）。

评估仅在评估期间出现且训练期间未见的新项目分段。三种项目表示方法的表现显示在表3a中。

与两个基线相比，语义ID提高了尾部项目的泛化能力，对头部项目是中性的（头部的物品单个都具备非常丰富的行为，不需要依靠聚类来提升模型对他们的学习感知），并且对躯干项目略有裨益。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_149035_CjM-6Qe0K0fT-LmZ_1744194121?w=1150&h=408&type=image/png)

### 物品表征随时间变化的分析

分析背景：随着时间的迁移，物品ID的进场和退场都非常频繁，如下图，6天之后，新广告注册旧广告下线，离开了系统，当初100%的物品id空间，就只剩1半了。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_487409_aR2RA3rhhg7EqB9V_1744193626?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

为了验证这种短时间大量的物品ID的进场和退场，对于物品ID的嵌入表征学习是否有影响，论文取训练周期结束前42-48小时的NE，并减去训练最后六小时的NE（？），得到的结果越小越好，如下所示：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_423576_AC1VG20LE94ovTJR_1744209788?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

可以看出来语义ID的收益跟唯一的物品ID的收益是相近的，说明语义ID还是可以抵抗这样的物品进退场的影响；

为了进一步验证，论文在20天的时间里对RH和SemID模型进行训练，并将它们与仅在最后四天训练的相应模型进行比较。如下表所示，与随机哈希相比，语义ID在更长时间的训练数据上的性能扩展得更好。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_756853_ShhOW7oIjTb0LY4v_1744210378?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

## 4.3 物品表征分析

希望看到由语义ID产生的基于语义的划分，是比随机哈希产生的随机划分来说，更适合推荐场景的。

当多个物品被分配到同一个分区时，它们会通过嵌入查找模块，映射到相同的嵌入向量。我们将这个嵌入向量视为由各个嵌入模型学习到的每个物品嵌入的总结。虽然我们在本文中为说明目的而对单个嵌入模型进行拟合，但在工业级设置中，IE（嵌入）是不切实际的。具有较低分区内部嵌入方差和较高分区间距离的分区可以被视为对个体嵌入更有效的一个总结。我们计算了IE模型学习到的嵌入的RH（随机哈希）和SemID（语义ID）分区的这些指标。

在这个实验中，论文将碰撞因子设置为5。因此，RH和SemID分区的结果簇平均包含5个项目。然而，由于语义ID是由RQ-VAE模型学习到的潜在代码，所以结果簇的大小高度可变。我们计算了两组语义ID簇的指标，一组是每个包含4-10个小簇，另一组是前1000个大簇，其中每个簇包含数千个项目。下表包含平均方差和平均成对距离，括号内为标准差。这些指标在嵌入embeddingsize的维度上取平均，以产生用于比较的单一标量。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_842_Fj1vR5eDUjZsL5Rw_1744210505?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

与随机哈希相比，语义ID分区产生的簇内方差较低。

然而，得到的成对距离的结论确不太容易解释了。论文假设前1000个簇之间低成对距离的原因是RQ-VAE在数据密度最高的区域放置了多个质心（无监督聚类的典型特点），以最小化模型损失。

## 4.4 用户历史行为建模

论文探讨了语义ID对用户历史建模的影响。语义ID的一个很大作用是理解并总结用户历史行为。

我们发现，使用语义ID和基于情境化的注意力聚合模块（PMA或Transformer）相比于不情境化序列的建模基线（绕过）带来了巨大的收益。如下所示：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_156411_pYVKx8x3G2ULzEy__1744211227?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

进一步，论文用4个指标来观测，为什么基于注意力机制的Transformer或者PMA，可以放大这样的NE增益；

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_171393_-9rhwYYuWSAGve2m_1744211436?w=968&h=256&type=image/png)

分别是第一个（最近的token）的平均注意力分数，填充平均注意力机制，注意力分数熵，token的自注意力评分分数，对比随机hash来说，结果如下：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_210964_HkVkiun0-ZLeHM5n_1744211670?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

结论如下：

1.  使用语义ID训练的模型具有较低的填充自注意力和填充标记注意力，并且在序列中第一个源标记上的注意力分数更高。这意味着基于语义ID的模型更重视高信号标记（即序列中的第一个和最近的项目，而不是早期且可能过时的标记或填充标记）；

2.  在整个序列上的注意力分数分布更为集中（即熵更低）；

3.  对于Transformer来说，对其他标记的重视程度高于self-attention的注意力分数；

这些都说明，语义ID建模之后的嵌入表征能够带来更好的模型性能

# 五、在线生产环境使用

## 5.1 生产环境总体说明

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_216927_rA3K2UbMaV7CuGaZ_1744212276?w=976&h=442&type=image/png)

### 离线RA-VAE说明

RQ-VAE模型在Meta上用于广告排名的内容理解（CU）模型上进行训练。

这些CU模型在公共的CC100数据集Conneau（2020年）上预训练，然后在内部广告数据集上进行微调。我们从过去三个月的数据中抽样广告ID及其相应的内容嵌入，并离线训练RQ-VAE模型。

对于生产模型，我们用L=6和K=2048来训练RQ-VAE，语义ID遵循前面的prefix-5gram的设计，其中embedding的表，大概是5000w的量级。训练完成后，我们使用一个冻结的RQ-VAE检查点进行在线服务。

### 在线语义ID服务

在广告创建时，我们处理广告内容信息并提供给CU模型。

然后通过RQ-VAE模型传递输出的CU嵌入，该模型计算每个原始ID的语义ID信号；该信号随后存储在数据库中。

在特征生成阶段，目标物品原始ID和用户参与原始ID历史记录通过来自实体数据存储的语义ID信号得到增强，以产生语义特征。

当服务请求到达时，预计算的特征被提取并传递给下游的排名模型。

## 5.2 生产环境效果

从不同的内容嵌入源（包括文本、图像和视频）创建了六个稀疏特征和一个顺序特征，并在下表中报告了旗舰Meta广告排名模型的NE增益。在Meta广告排名中，一个大于0.02%的离线NE增益，就被认为是有意义的。下表当中，NE的离线指标提升远高于0.02%

总的来说，在多个广告排名模型中，加入语义ID特性后，我们的主要在线指标性能提升了0.15%。

由于Meta广告推荐系统服务数十亿用户，并且是公司中最优化的模型之一，因此0.15%的在线性能提升被认为是显著的。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_205522_MLrXvGv9X1VnncQb_1744212576?w=960&h=280&type=image/png)

## 5.3 语义的预测相似性评估

为了确保使用语义ID时的稳健交付性能，我们必须确保排名模型的行为与我们系统中项目之间的语义相似性关系保持一定程度的连续性（或相关性）。

为了衡量这种相关性，我们进行了一项在线A/B测试，在该测试中，我们选择了一组由系统推荐给用户的物品S。对于给定的用户，有50%的概率，我们会通过随机交换S中的一个物品和具有相同前缀的不同物品（来自语义ID）来将集合S变为S'。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_678053_fjCcVB9VjlUSZhh9_1744212641?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

从结果上看，利用越是相同的前缀的语义ID的物品，点击率下降的越少：

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_753825_ofTTsbrSk9e67MF9_1744212712?w=956&h=698&type=image/png)

## 5.4 影子广告AA测试

随机哈希的排名模型的另一个缺点是永远存在模型预测方差，导致下游广告投放的方差。

具体来说，可以创建一个具有不同原始项目ID的项目副本。然后，原始项目和项目副本都进入推荐系统。由于哈希后原始项目和项目副本的嵌入将不同，模型预测和投放系统行为也会有所不同。我们将这种现象称为A/A方差，其中“A/A”表示我们考虑的是原始项目的精确副本。这种方差是不希望出现的，因为它降低了下游广告排序的稳健性和系统精准定位正确受众的能力。

语义ID有助于通过消除随机哈希引起的随机性，来减少A/A方差——精确副本或非常相似的项目通常会有相同的k前缀语义ID。

![](https://wdoc-76491.picgzc.qpic.cn/MTY4ODg1MDU3MDk1MzEyNA_579867_FIgql-F4OQoZ1gaZ_1744213028?imageMogr2/thumbnail/3200x%3E/ignore-error/1)

实验通过生成影子广告，如果两个广告的排序差异也就是如上的AAR指标越低，说明推荐系统的推荐方差是越小，模型越稳定可靠。

结果也是如此，包含六个语义ID稀疏特征的生产模型中，与没有这六个特征的相同模型相比，平均AAR减少了43%。

# 六、对业务场景的启发

## 模型优化

去年pltv放量的一个实验，核心是修复好了流量主ID这个场景特征，说明各种ID特征的有效性。进一步的，这些ID目前大多是随机hash的方法来建模，忽略了不同物品的语义相似，可以引入语义ID的方式提高模型学习感知。

## 小程序场景理解

尤其是，在目前场景理解建模的背景上，我们通过跟平台、商数合作，建模小程序的语义ID（避免人工打标的主观差异太大，也避免大模型打标的不准确和prompt敏感问题）。

依据这样的语义ID，对于后续变现的策略算法迭代，变现的模型优化都是有帮助的。

## 推荐感

相关性建模一直是个难点，与否可以通过语义ID的方法，天然引入码本前缀的topK重合度来建模文章内容跟广告词的关联关系。

# 尾语

电影《降临》，讲述了一个外星人来到地球，该外星人的语言是有时间概念的，并且对于他们来说，时间是一个整体，可以同时知晓未来和过去，他们来到地球，是想给人们灾难预警，也是希望3000w年后人类来帮助他们解决灾难。

一个语言学家通过学习他们的语言，获得了跟他们一样的思维认知，将时间当做是一个整体并行的，而不是线性的时间，因此可以同时知晓过去，现在和未来，最终帮助人类和外星人破除危机。

现在已经有一个高维的智能生命出现来到地球了，对于他来说，时间是永恒的，但是我们却用人类的语言和时间思维来跟他们交流，如果哪天他的语言进化了？时间不再交流、决策的考虑因素了？

“盖将其自变者而观之，则天地曾不能以一瞬”

“自其不变者而观之，则物与我皆无尽藏也，而又何羡乎”

问题：这个高维的智能生命是？