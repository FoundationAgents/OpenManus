# Qwen2.5-VL 论文详细介绍

**文档说明**：本文档基于Qwen2.5-VL技术报告编写，按照背景、核心贡献、核心贡献细节详细说明、实验分析与结论共5个章节结构组织，旨在提供全面、详细的中文介绍。

---

## 1. 背景

### 1.1 多模态大语言模型的发展现状

大型视觉语言模型（LVLMs）代表了人工智能领域的关键突破，标志着多模态理解和交互的变革性方法。通过将视觉感知与自然语言处理无缝集成，这些先进模型正在从根本上重塑机器如何跨不同领域解释和分析复杂信息。

尽管多模态大语言模型取得了显著进展，但当前这些模型的能力可以被比作夹心饼干的中层——在各种任务上表现良好，但在卓越性能方面仍有不足。细粒度视觉任务构成了这个类比的基础层。在Qwen2.5-VL这一版本中，我们致力于探索细粒度感知能力，旨在为LVLMs建立坚实的基础，并为实际应用创建代理放大器。

### 1.2 现有模型的局限性

尽管取得了显著进展，视觉语言模型目前面临着发展瓶颈，包括：
- 计算复杂性
- 有限的上下文理解能力
- 较差的细粒度视觉感知
- 在不同序列长度下性能不一致

### 1.3 Qwen2.5-VL的定位

Qwen2.5-VL是Qwen视觉语言系列的最新旗舰模型，在基础能力和创新功能方面都展示了显著进步。该模型通过增强的视觉识别、精确的对象定位、强大的文档解析和长视频理解能力，实现了与世界理解和交互的重大飞跃。

![](images/3aa1d0c049493603092a42ad25b4afacebf8443edd87f90886d9f850eca07de6.jpg)

![](images/aadc116f02ab2f4bb3d8971ea36ea9bd209d1afcfc959749bc2f97fde610f0c9.jpg)

![](images/8587745d4fe258ea570836d039443ea1e37bc2a6d6ae77b371a80965f7ddfd72.jpg)

![](images/412a95c79d13d4b59a307adbe67501edb98dea2365551f3fa0f1243ae2b4fce6.jpg)

## 2. 核心贡献

### 2.1 技术架构创新

Qwen2.5-VL在技术架构上做出了四个主要贡献：

1. **窗口注意力机制**：在视觉编码器中实现窗口注意力以优化推理效率
2. **动态FPS采样**：将动态分辨率扩展到时间维度，实现跨不同采样率的全面视频理解
3. **时间对齐的MRoPE**：在时间域升级MRoPE，与绝对时间对齐，促进更复杂的时间序列学习
4. **高质量数据策展**：在预训练和监督微调方面做出重大努力，将预训练语料库从1.2万亿token扩展到4.1万亿token

### 2.2 模型特色能力

Qwen2.5-VL的突出特点包括：

- **强大的文档解析能力**：将文本识别升级为全文档解析，在处理多场景、多语言和各种内置（手写、表格、图表、化学公式和乐谱）文档方面表现出色
- **跨格式的精确对象定位**：在检测、指向和计数对象方面解锁了改进的准确性，适应绝对坐标和JSON格式以进行高级空间推理
- **超长视频理解和细粒度视频定位**：将原生动态分辨率扩展到时间维度，增强了对持续数小时的视频的理解能力，同时以秒级精度提取事件片段
- **增强的计算机和移动设备代理功能**：利用先进的定位、推理和决策能力，在智能手机和计算机上提升模型的代理功能

## 3. 核心贡献细节详细说明

### 3.1 模型架构设计

Qwen2.5-VL的整体模型架构由三个组件组成：

![](images/7cc04b7f5204168923f9961aa125093d2a23068a92e7758f97ee35b9afecdcb1.jpg)

*图1：Qwen2.5-VL框架展示了视觉编码器和语言模型解码器的集成，用于处理多模态输入，包括图像和视频。视觉编码器设计用于处理原生分辨率的输入并支持动态FPS采样。不同大小的图像和不同FPS率的视频帧被动态映射到不同长度的token序列。值得注意的是，MRoPE将时间ID与绝对时间沿时间维度对齐，使模型能够更好地理解时间动态，如事件节奏和精确时刻定位。处理后的视觉数据随后被馈送到Qwen2.5 LM解码器。我们重新设计了视觉变换器（ViT）架构，结合了先进组件，如带有SwiGLU激活的FFN、用于归一化的RMSNorm和基于窗口的注意力机制，以增强性能和效率。*

#### 3.1.1 大语言模型

Qwen2.5-VL系列采用大语言模型作为其基础组件。该模型使用Qwen2.5 LLM的预训练权重进行初始化。为了更好地满足多模态理解的需求，我们将1D RoPE（旋转位置嵌入）修改为与绝对时间对齐的多模态旋转位置嵌入。

#### 3.1.2 视觉编码器

Qwen2.5-VL的视觉编码器采用重新设计的Vision Transformer（ViT）。为了应对由于原生分辨率输入导致的训练和推理期间计算负载不平衡的挑战，我们重新设计了ViT架构。

**关键创新**：
- **窗口注意力机制**：在大多数层中引入窗口注意力，确保计算成本随补丁数量线性扩展而非二次扩展
- **2D旋转位置嵌入**：采用2D RoPE有效捕捉2D空间中的空间关系
- **3D补丁分区**：对于视频数据，将两个连续帧分组在一起，显著减少输入语言模型的token数量

#### 3.1.3 MLP视觉语言合并器

为了解决图像特征长序列带来的效率挑战，我们采用简单而有效的方法在将特征输入大语言模型之前压缩特征序列。具体来说，不是直接使用Vision Transformer提取的原始补丁特征，我们首先将空间相邻的四组补丁特征分组。然后将这些分组特征连接起来，并通过两层多层感知器（MLP）将它们投影到与LLM中使用的文本嵌入对齐的维度。

### 3.2 原生动态分辨率和帧率

Qwen2.5-VL在空间和时间维度上都引入了进步，以有效处理多样化的多模态输入。

#### 3.2.1 空间域

Qwen2.5-VL动态地将不同大小的图像转换为具有相应长度的token序列。与归一化坐标的传统方法不同，我们的模型直接使用输入图像的实际维度来表示边界框、点和其他空间特征。这使得模型能够固有地学习尺度信息，提高其处理不同分辨率图像的能力。

#### 3.2.2 时间域

对于视频输入，Qwen2.5-VL结合了动态帧率（FPS）训练和绝对时间编码。通过适应可变帧率，模型可以更好地捕捉视频内容的时间动态。与其他方法不同，我们引入了一种新颖而高效的策略，直接将MRoPE ID与时间戳对齐。这种方法使模型能够通过时间维度ID之间的间隔来理解时间节奏，而无需任何额外的计算开销。

### 3.3 与绝对时间对齐的多模态旋转位置嵌入

位置嵌入对于在视觉和语言模态中建模序列数据至关重要。基于Qwen2-VL中引入的多模态旋转位置嵌入（MRoPE），我们扩展了其能力以更好地处理视频中的时间信息。

**MRoPE改进**：
- Qwen2-VL中的MRoPE将位置嵌入分解为三个不同的组件：时间、高度和宽度，以有效建模多模态输入
- 对于文本输入，所有三个组件使用相同的位置ID，使MRoPE在功能上等同于传统的1D RoPE
- 对于图像，时间ID在视觉token中保持恒定，而高度和宽度组件根据每个token在图像中的空间位置分配唯一ID
- 当处理视频时，时间ID为每个帧递增，而高度和宽度组件遵循与静态图像相同的分配模式

**关键改进**：Qwen2.5-VL引入了一个关键改进：将MRoPE的时间组件与绝对时间对齐。通过利用时间ID之间的间隔，模型能够学习跨不同FPS采样率视频的一致时间对齐。

### 3.4 预训练数据策略

与Qwen2-VL相比，我们显著扩展了预训练数据的量，从1.2万亿token增加到约4万亿token。我们的预训练数据集通过多种方法构建，包括清理原始网络数据、合成数据等。

**数据集组成**：
- 图像字幕
- 交错图像文本数据
- 光学字符识别（OCR）数据
- 视觉知识（如名人、地标、动植物识别）
- 多模态学术问题
- 定位数据
- 文档解析数据
- 视频描述
- 视频定位
- 基于代理的交互数据

### 3.5 监督微调策略

监督微调（SFT）阶段采用两阶段过滤管道，确保数据质量和多样性。

#### 3.5.1 阶段1：领域特定分类

在初始阶段，我们使用Qwen2-VL-Instag（从Qwen2-VL-72B派生的专门分类模型）执行问题-答案（QA）对的分层分类。该模型将QA对组织到八个主要领域，如编码和规划，这些领域进一步分为30个细粒度子类别。

#### 3.5.2 阶段2：领域定制过滤

第二阶段涉及领域定制过滤，整合了基于规则和基于模型的方法，以全面增强数据质量。

**基于规则的过滤**：
- 消除低质量或有问题的条目
- 识别和删除重复模式
- 排除包含不完整、截断或格式不正确的响应的条目
- 丢弃不相关或可能导致有害输出的查询和答案

**基于模型的过滤**：
- 利用在Qwen2.5-VL系列上训练的奖励模型
- 跨多个维度评估多模态QA对
- 评估查询的复杂性和相关性
- 评估答案的正确性、完整性、清晰度、与查询的相关性和有用性

### 3.6 拒绝采样增强推理

为了补充我们的结构化数据过滤管道，我们采用拒绝采样作为改进数据集和增强视觉语言模型推理能力的策略。这种方法对于需要复杂推理的任务特别关键，如数学问题解决、代码生成和领域特定的视觉问答（VQA）。

**拒绝采样过程**：
- 从包含真实注释的数据集开始
- 使用Qwen2.5-VL模型的中间版本评估生成的响应与真实答案
- 仅保留模型输出与预期答案匹配的样本
- 应用额外约束过滤掉不良输出

## 4. 实验分析与结论

### 4.1 与最先进模型的比较

Qwen2.5-VL在多个基准测试中表现出色，与当前最先进的模型相比具有竞争力：

![](images/df7717b211a2373770c82f0050459f1cc7f423a004da0cf713815afdb8f50d44.jpg)

![](images/f01cd3a1300b0c73e2bf736f2263fb47d9e52b24fd36a59819b92d0c7f79edb8.jpg)

#### 4.1.1 大学级问题

- **MMMUval**：Qwen2.5-VL-72B达到70.2分，与GPT-4o（69.1）和Claude-3.5 Sonnet（68.3）相当
- **MMMU-Pro**：Qwen2.5-VL-72B达到51.1分，接近GPT-4o（51.9）和Claude-3.5 Sonnet（51.5）

#### 4.1.2 数学能力

- **MathVista mini**：Qwen2.5-VL-72B达到74.8分，超过GPT-4o（63.8）和Claude-3.5 Sonnet（67.7）
- **MATH-Vision**：Qwen2.5-VL-72B达到38.1分，显著超过GPT-4o（30.4）
- **MathVerse mini**：Qwen2.5-VL-72B达到57.6分，超过GPT-4o（50.2）

#### 4.1.3 通用视觉问答

- **MegaBench**：Qwen2.5-VL-72B达到51.3分，接近GPT-4o（54.2）和Claude-3.5 Sonnet（52.1）
- **MMBench-EN**：Qwen2.5-VL-72B达到88.6分，超过GPT-4o（83.4）和Claude-3.5 Sonnet（82.6）
- **MMStar**：Qwen2.5-VL-72B达到70.8分，超过GPT-4o（64.7）和Claude-3.5 Sonnet（65.1）

### 4.2 不同规模模型的性能

Qwen2.5-VL提供三种规模，满足从边缘AI到高性能计算的多样化用例：

- **Qwen2.5-VL-72B**：旗舰模型，在文档和图表理解方面表现特别出色，与GPT-4o和Claude 3.5 Sonnet等最先进模型相当
- **Qwen2.5-VL-7B**：在资源受限环境中提供强大能力，优于同类竞争对手
- **Qwen2.5-VL-3B**：在边缘设备上提供高效性能，同时保持强大的多模态理解能力

![](images/965216cee11c79d7dee42ad17c939703de148eddbca424d01d54cef32f4f0b7d.jpg)

![](images/173ae54bfa5d57cf2bc0f2f5f79fe226cafcdb268f9bf5e1c0d61fdae88c281c.jpg)

![](images/a8ddb4b2b4d7282a8bc1d7ef0588699859f9b7bf72306cfd64e887d4f8bc4aec.jpg)

![](images/ac48591f66d1d18f924a9e67f3e2c75d8b286585114f7ad8aeb7f40aaaaea3a5.jpg)

![](images/b141662e75cc906a85ba551144118caf46fd8e95ec00fc128a6de5bab0f447d1.jpg)

![](images/5a6689702b53b8c421daed97ad4746aad8e2e9cc14f469ea142a97a217ffea70.jpg)

![](images/cbe68dccfad98167482230415ba521a828f4ff55c08dddf5b402f006242daa48.jpg)

![](images/e716a573b3886fff65717ecca52313b4cae6395ddeb9053d6e045dcd4bfbaa56.jpg)

![](images/2d1bb8aa7a3f922c7dea9eadd01485277990dcee06432dd140c33eb9278de5f1.jpg)

### 4.3 关键优势总结

1. **原生分辨率处理**：能够处理不同大小的图像和不同持续时间的视频，无需传统归一化技术
2. **精确的空间和时间理解**：通过边界框或点精确定位对象，以及秒级事件定位
3. **强大的文档解析**：从发票、表格和表格中提取强大的结构化数据，以及图表、图表和布局的详细分析
4. **增强的代理功能**：作为交互式视觉代理，能够在操作计算机和移动设备等现实场景中进行推理、工具使用和任务执行
5. **跨领域泛化**：在不需要任务特定微调的情况下实现跨领域的强泛化

### 4.4 结论

Qwen2.5-VL代表了多模态大语言模型发展的一个重要里程碑。通过创新的架构设计、先进的位置编码技术和精心策划的训练数据，该模型在视觉理解、文档解析、视频分析和代理功能方面都达到了最先进的性能水平。

Qwen2.5-VL不仅展示了在静态图像和文档理解方面的卓越能力，还作为交互式视觉代理，能够在现实场景中进行推理、工具使用和任务执行。该模型的开源性质进一步促进了多模态AI研究的发展，为更广泛的应用和创新奠定了基础。

随着多模态AI技术的不断发展，Qwen2.5-VL为未来更智能、更通用的视觉语言模型设定了新的基准，为实现真正的人工通用智能迈出了重要一步。