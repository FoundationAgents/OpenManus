{
    "pdf_info": [
        {
            "para_blocks": [
                {
                    "bbox": [
                        130,
                        107,
                        479,
                        149
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                130,
                                107,
                                479,
                                149
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        130,
                                        107,
                                        479,
                                        149
                                    ],
                                    "type": "text",
                                    "content": "DeepStack: Deeply Stacking Visual Tokens is Surprisingly Simple and Effective for LMMs"
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        170,
                        183,
                        431,
                        208
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                170,
                                183,
                                431,
                                208
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": "Lingchen Meng "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{1,2*}"
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": " Jianwei Yang "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{3*}"
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": " Rui Tian "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{1,2}"
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": " Xiyang Dai "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{3}"
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": " Zuxuan Wu "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{1,2\\dagger}"
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": " Jianfeng Gao "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{3\\dagger}"
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "text",
                                    "content": " Yu-Gang Jiang "
                                },
                                {
                                    "bbox": [
                                        170,
                                        183,
                                        431,
                                        208
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{1,2}"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        146,
                        209,
                        464,
                        220
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                146,
                                209,
                                464,
                                220
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        146,
                                        209,
                                        464,
                                        220
                                    ],
                                    "type": "text",
                                    "content": "<sup>1</sup>Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        153,
                        220,
                        457,
                        233
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                153,
                                220,
                                457,
                                233
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        153,
                                        220,
                                        457,
                                        233
                                    ],
                                    "type": "inline_equation",
                                    "content": "^{2}"
                                },
                                {
                                    "bbox": [
                                        153,
                                        220,
                                        457,
                                        233
                                    ],
                                    "type": "text",
                                    "content": "Shanghai Collaborative Innovation Center of Intelligent Visual Computing"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        258,
                        233,
                        354,
                        244
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                258,
                                233,
                                354,
                                244
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        258,
                                        233,
                                        354,
                                        244
                                    ],
                                    "type": "text",
                                    "content": "<sup>3</sup>Microsoft Corporation"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        221,
                        249,
                        387,
                        261
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                221,
                                249,
                                387,
                                261
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        221,
                                        249,
                                        387,
                                        261
                                    ],
                                    "type": "text",
                                    "content": "https://deepstack-v1.github.io/"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "type": "image",
                    "bbox": [
                        106,
                        272,
                        504,
                        407
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                272,
                                504,
                                407
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        272,
                                        504,
                                        407
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                272,
                                                504,
                                                407
                                            ],
                                            "type": "image",
                                            "image_path": "54a12334f3b2d34a8af6c747d17b90a688941c489ccb5573f368ba2ded09dd96.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 6,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                104,
                                415,
                                506,
                                504
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        415,
                                        506,
                                        504
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                415,
                                                506,
                                                504
                                            ],
                                            "type": "text",
                                            "content": "Figure 1: Left: Conventional large multimodal models (LMMs) string all visual tokens into a sequence for high- and low-resolution images. Middle: Our DeepStack LMMs stack the tokens into a grid and infuse them into the first and middle transformer layers from bottom to top (↑↑↑↑) simply using a residual connection. With no architecture modification and context length increasing, our model can handle multiple times more visual tokens as inputs. Right: We apply DeepStack separately to Vicuna-7B (DeepStack-L) and CLIP ViT-L (DeepStack-V). Our models can take "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                415,
                                                506,
                                                504
                                            ],
                                            "type": "inline_equation",
                                            "content": "4 \\times"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                415,
                                                506,
                                                504
                                            ],
                                            "type": "text",
                                            "content": " more visual tokens, and significantly outperforms the sequence LMM with same context length and rival the one using a much longer context, over a wide range of benchmarks."
                                        }
                                    ]
                                }
                            ],
                            "index": 7,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        281,
                        516,
                        329,
                        529
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                281,
                                516,
                                329,
                                529
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        281,
                                        516,
                                        329,
                                        529
                                    ],
                                    "type": "text",
                                    "content": "Abstract"
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        140,
                        540,
                        470,
                        696
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                140,
                                540,
                                470,
                                696
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        140,
                                        540,
                                        470,
                                        696
                                    ],
                                    "type": "text",
                                    "content": "Most large multimodal models (LMMs) are implemented by feeding visual tokens as a sequence into the first layer of a large language model (LLM). The resulting architecture is simple but significantly increases computation and memory costs, as it has to handle a large number of additional tokens in its input layer. This paper presents a new architecture DeepStack for LMMs. Considering "
                                },
                                {
                                    "bbox": [
                                        140,
                                        540,
                                        470,
                                        696
                                    ],
                                    "type": "inline_equation",
                                    "content": "N"
                                },
                                {
                                    "bbox": [
                                        140,
                                        540,
                                        470,
                                        696
                                    ],
                                    "type": "text",
                                    "content": " layers in the language and vision transformer of LMMs, we stack the visual tokens into "
                                },
                                {
                                    "bbox": [
                                        140,
                                        540,
                                        470,
                                        696
                                    ],
                                    "type": "inline_equation",
                                    "content": "N"
                                },
                                {
                                    "bbox": [
                                        140,
                                        540,
                                        470,
                                        696
                                    ],
                                    "type": "text",
                                    "content": " groups and feed each group to its aligned transformer layer from bottom to top, as illustrated in Fig. 1. Surprisingly, this simple method greatly enhances the power of LMMs to model interactions among visual tokens across layers but with minimal additional cost. We apply DeepStack to both language and vision transformer in LMMs, and validate the effectiveness of DeepStack LMMs with extensive empirical results. Using the same context length, our DeepStack 7B and 13B parameters surpass their counterparts by 2.7 and 2.9 on average across 9 benchmarks, respectively. Using only one-fifth of the context length, DeepStack rivals closely to the counterparts"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        122,
                        702,
                        296,
                        714
                    ],
                    "type": "page_footnote",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                122,
                                702,
                                296,
                                714
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        122,
                                        702,
                                        296,
                                        714
                                    ],
                                    "type": "text",
                                    "content": "* Equal contributions; † Corresponding authors."
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        105,
                        731,
                        192,
                        742
                    ],
                    "type": "footer",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                731,
                                192,
                                742
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        731,
                                        192,
                                        742
                                    ],
                                    "type": "text",
                                    "content": "Preprint. Under review."
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        14,
                        215,
                        36,
                        555
                    ],
                    "type": "aside_text",
                    "angle": 270,
                    "lines": [
                        {
                            "bbox": [
                                14,
                                215,
                                36,
                                555
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        14,
                                        215,
                                        36,
                                        555
                                    ],
                                    "type": "text",
                                    "content": "arXiv:2406.04334v1 [cs.CV] 6 Jun 2024"
                                }
                            ]
                        }
                    ],
                    "index": 12
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 0
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        140,
                        72,
                        470,
                        128
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                140,
                                72,
                                470,
                                128
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        140,
                                        72,
                                        470,
                                        128
                                    ],
                                    "type": "text",
                                    "content": "that use the full context length. These gains are particularly pronounced on high-resolution tasks, e.g., 4.2, 11.0, and 4.0 improvements on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. We further apply DeepStack to vision transformer layers, which brings us a similar amount of improvements, 3.8 on average compared with LLaVA-1.5-7B."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        105,
                        151,
                        191,
                        164
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                151,
                                191,
                                164
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        151,
                                        191,
                                        164
                                    ],
                                    "type": "text",
                                    "content": "1 Introduction"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        176,
                        506,
                        254
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                176,
                                506,
                                254
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        176,
                                        506,
                                        254
                                    ],
                                    "type": "text",
                                    "content": "With the tremendous advancements in large language models (LLMs) [62, 63, 87, 6, 6, 65, 59], we have witnessed a surge of efforts of developing large multimodal models (LMMs) [51, 88]. To connect vision and language models for LMMs, a conventional way is transforming images into a number of visual features using pretrained vision encoders (e.g., CLIP [61]), and flattening them to a sequence of \"language tokens\" which are then fed into an LLM. With sufficient alignment and instruction tuning, the entire system can demonstrate a broad conversational capability for multimodal inputs [51]."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        258,
                        506,
                        422
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                258,
                                506,
                                422
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        258,
                                        506,
                                        422
                                    ],
                                    "type": "text",
                                    "content": "To incorporate visual inputs, it usually requires the LMMs to handle a large number of visual tokens as the prefix tokens in addition to the original language prompts. This inevitably introduces a tremendous memory and compute overhead into the LLMs, which is particularly significant when it comes to high-resolution images and multi-frame videos. Several previous works attempt to mitigate this issue by proposing various token compression strategies. A straightforward way is to reduce the number of tokens with spatial grouping [70, 47]. Instead of pooling vision tokens, a few work instead to concatenate local tokens along the feature dimension to preserve visual information [11, 48]. Moreover, other works seek more sophisticated token resampling, such as Q-Former [43], Perceiver [4] and Abstractor [8], etc. In MM1 [57], the researchers performed an extensive analysis of these approaches and found no significant discrepancies among them. Despite the huge effort, all these works inherently sacrifice fine-grained visual information to reach the trade-off between the compute overhead and the information flow into LLMs, which is arguably problematic for high-resolution images and videos. Most recently, a few works [22, 48, 50, 19, 20] proposed multi-crop strategies and string several times more visual tokens to support high-resolution scenarios, while at the cost of substantial overhead."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        427,
                        506,
                        570
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                427,
                                506,
                                570
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        427,
                                        506,
                                        570
                                    ],
                                    "type": "text",
                                    "content": "All current efforts to wire vision with LLMs follow the routine in which visual tokens are always rolled together as a 1d sequence, and fed into the first layer of LLMs as inputs. In this work, we step outside the box and question whether we can find a better strategy to handle the large number of visual tokens regarding both efficacy and efficiency. Instead of examining the LLMs in a traditional left-to-right orientation, we adopt a novel bottom-to-top perspective, revealing that they constitute a hierarchical arrangement of transformer layers. Based on this observation, we propose DeepStack, a simple, yet novel way of feeding visual tokens into LLMs. As shown in Fig. 1, instead of putting the long sequence of visual tokens from left to right, we restructure the visual tokens into a layered stack, where each layer of the stack is connected to one layer in the LLMs by simple residual connection. As a result, with the context length unchanged, we can feed into LLMs several times more visual tokens to handle complex visual inputs. Meanwhile, the combination of per-layer parallel attention and layer-by-layer progression can effectively leverage the LLMs' capacity for modeling the dependencies of visual tokens."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        574,
                        506,
                        641
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                574,
                                506,
                                641
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        574,
                                        506,
                                        641
                                    ],
                                    "type": "text",
                                    "content": "To examine the effectiveness of our method, we apply it to two representative LMMs, LLaVA-1.5 [51] and LLaVA-Next [50]. Extensive empirical results demonstrate the effectiveness of our method. More specifically, with the same setting of LLaVA-1.5, our model can achieve significant performance gain across a wide range of benchmarks. In particular, our model brings 4.2, 11.0, and 4.0 performance gains on TextVQA, DocVQA, and InfoVQA compared to LLaVA-1.5-7B, respectively. To summarize, our main contributions are three-fold:"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        105,
                        651,
                        504,
                        722
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 8,
                    "blocks": [
                        {
                            "bbox": [
                                105,
                                651,
                                504,
                                684
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        105,
                                        651,
                                        504,
                                        684
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                105,
                                                651,
                                                504,
                                                684
                                            ],
                                            "type": "text",
                                            "content": "- We propose a simple yet effective DeepStack strategy for connecting vision and language in the context of LMMs. This new strategy introduces no architecture change while significantly increasing the number of tokens LLMs can take."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                105,
                                689,
                                504,
                                722
                            ],
                            "type": "text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        105,
                                        689,
                                        504,
                                        722
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                105,
                                                689,
                                                504,
                                                722
                                            ],
                                            "type": "text",
                                            "content": "- With the DeepStack strategy, we present our new model DeepStack, and compare it with LMMs across a wide range of multimodal tasks. Our model demonstrates consistent improvement over the baseline methods, in particular for high-resolution tasks."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        }
                    ],
                    "sub_type": "text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "2"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 1
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        105,
                        72,
                        504,
                        95
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                72,
                                504,
                                95
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        72,
                                        504,
                                        95
                                    ],
                                    "type": "text",
                                    "content": "- We further conduct comprehensive ablation studies on different aspects of our proposed method, which provide useful guidance and insights behind the design choices."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        111,
                        504,
                        156
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                111,
                                504,
                                156
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        111,
                                        504,
                                        156
                                    ],
                                    "type": "text",
                                    "content": "Finally, although we only demonstrate the effectiveness of our proposed method in the context of LMMs, we note that this simple strategy could be generalized to any models or tasks built on top of transformer layers. We hope this new design could shield new lights and open up new exploratory directions regarding how to wire vision encoders and LLMs in large multimodal models."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        105,
                        186,
                        203,
                        201
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                186,
                                203,
                                201
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        186,
                                        203,
                                        201
                                    ],
                                    "type": "text",
                                    "content": "2 Related Works"
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        220,
                        506,
                        474
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                220,
                                506,
                                474
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        220,
                                        506,
                                        474
                                    ],
                                    "type": "text",
                                    "content": "Large Language Models (LLMs). Recently, natural language processing (NLP) has witnessed significant progress, particularly with the advent of large language models (LLMs) [74, 87, 64, 6]. Building on the foundational architecture of Transformers [75], language models [18, 74, 87, 64, 6, 39] have demonstrated strong scalability through the pretraining-then-finetuning paradigm. Specifically, BERT [18] utilizes the transformer encoder and introduces a masked language modeling task to pre-train the model on vast unlabelled data, showing excellent performance after fine-tuning on downstream tasks. Other follow-ups [39, 36] continue along the lines of BERT, constantly refining and optimizing its performance. The T5 [64] series further unifies different NLP tasks within an encoder-decoder architecture, demonstrating effectiveness across dozens of language understanding tasks. Meanwhile, the GPT [62, 63, 4] series employs simple decoder-only transformers to pretrain the language model using a unified next-token prediction paradigm. This approach shows remarkable scalability in terms of both model size and data scale. To enhance instruction-following abilities, InstructGPT [59] and ChatGPT emphasize the importance of instruction tuning and Reinforcement Learning from Human Feedback (RLHF). These models exhibit excellent capabilities in open-domain conversation tasks, ranging from text generation to question answering. In response to ChatGPT, recent works [74, 15, 38] have made significant efforts in developing an open-source LLMs community. Building on the success of the LLaMA [74] series foundation model, Alpaca [71], Vicuna [15], and GPT-4-LLM [60] showcase the improvements brought by higher-quality instruction datasets. Other works [24, 27, 1, 86] take a different approach, aiming to achieve comparable performance with a much smaller set of parameters. The Phi [24, 27, 1] series revisits the importance of the pre-training corpus and achieves success with models containing around 3 billion parameters. In this paper, we develop our model based on Vicuna [15] and Phi-3 [1], aiming to equip the well-trained LLMs with informative visual tokens and a relatively small training effect."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        481,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                481,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        481,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "Large Multi-modal Models (LMMs). The success of CLIP [61] and its follow-ups [66, 28, 77] demonstrates the effectiveness of aligning vision and language modalities into a unified semantic space, showcasing promising capabilities in zero-shot classification tasks. More recently, Flamingo [3] and BLIP [44] have utilized visual perceivers [26] to resample visual tokens from image features as inputs for language models through cross-attention. BLIP-2 [42] and Instruct-BLIP [16] further incorporate this mechanism into large language models for tasks such as visual captioning and question-answering. Although visual perceivers can translate image features into a fixed set of visual tokens, they face constraints related to convergence costs and data requirements. In parallel, LLaVA and its follow-ups [13, 76, 47, 50, 49] achieved success in connecting vision and language using a simple projection module. It greatly simplifies the difficulties of alignment tasks and even achieves better performance with less training effort. However, due to the rigorous input resolution of pre-trained models, these directions meet difficulties on downstream tasks requiring finer-grained visual information, e.g. tasks relevant to OCR and documents. To alleviate this problem, recent works [48, 22, 21, 73, 89] utilize a mixture of experts (MOE) schemes to leverage different pretrained vision models, typically assembling the visual tokens along the feature dimension. Other attempts [85, 19, 50] split high-resolution images into multi-crop patches and merge them into a longer sequence, which significantly increases the training and evaluation cost. In this work, we conduct experiments on the projector-based connection framework and revisit the connection scheme that utilizes projected visual tokens for the input layer of LLMs. We find that the early layers of LLMs can also well process visual token inputs. Besides that, we propose a DeepStack scheme to stack finer-grained visual tokens to the early layers of LLMs, enhancing visual capabilities without introducing extra input tokens."
                                }
                            ]
                        }
                    ],
                    "index": 4
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "3"
                                }
                            ]
                        }
                    ],
                    "index": 5
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 2
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        109,
                        73,
                        302,
                        210
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                109,
                                73,
                                302,
                                210
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        109,
                                        73,
                                        302,
                                        210
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                109,
                                                73,
                                                302,
                                                210
                                            ],
                                            "type": "image",
                                            "image_path": "e66b92968fd38a48cabb4a2b18023f697091cb875ad04d39d6d0f8a5342ab36b.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                104,
                                223,
                                504,
                                285
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        223,
                                        504,
                                        285
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                223,
                                                504,
                                                285
                                            ],
                                            "type": "text",
                                            "content": "Figure 2: Architecture of DeepStack. The main innovation lies in the DeepStack strategy that infuses visual tokens into different layers. Left: DeepStack for LLMs. Given an input image, we feed the tokens extracted from the low-resolution version to the input layer of LLM. Considering the 2D nature of images, we extra the neighbors from the high-resolution version and reorganize them into DeepStack, which are then fed to the consequent layers in LLMs. Right: DeepStack for ViTs. We apply similar sampling strategy but feed the visual tokens into the ViT layers of vision encoder."
                                        }
                                    ]
                                }
                            ],
                            "index": 2,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "image",
                    "bbox": [
                        309,
                        72,
                        502,
                        211
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                309,
                                72,
                                502,
                                211
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        309,
                                        72,
                                        502,
                                        211
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                309,
                                                72,
                                                502,
                                                211
                                            ],
                                            "type": "image",
                                            "image_path": "afda93350a230a9f71b7fe0227d1c234f5ca6000344a181b68f8d246b1dad783.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_body"
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        105,
                        301,
                        181,
                        316
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                301,
                                181,
                                316
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        301,
                                        181,
                                        316
                                    ],
                                    "type": "text",
                                    "content": "3 DeepStack"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        325,
                        506,
                        414
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                325,
                                506,
                                414
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        325,
                                        506,
                                        414
                                    ],
                                    "type": "text",
                                    "content": "DeepStack is a versatile strategy that provides finer-grained visual information without increasing the visual context length for LMMs. It achieves this by dividing image feature extraction into two streams: a global-view stream that captures global information, and a high-resolution stream that enhances the global information by stacking dilated high-resolution image features across different layers of the LLMs. This dual-stream approach offers LMMs detailed visual features while maintaining efficiency. By leveraging this simple yet effective method, we build DeepStack, which significantly improves the ability of LMMs to process and comprehend fine-grained visual details. We illustrate DeepStack in Fig. 2 and propose a pseudo-code implementation in Algorithm. 1."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        426,
                        296,
                        437
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                426,
                                296,
                                437
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        426,
                                        296,
                                        437
                                    ],
                                    "type": "text",
                                    "content": "3.1 Preliminary: Large Multimodal Model"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        445,
                        504,
                        489
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                445,
                                504,
                                489
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        504,
                                        489
                                    ],
                                    "type": "text",
                                    "content": "Large Language Models (LLMs). LLMs [2, 11, 70, 74] are typically pre-trained on a huge amount of unlabeled text corpus using a transformer decoder-only architecture. The primary pre-training task is next-token prediction driving their learning process. Formally, the learning objective can be formulated as:"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        246,
                        491,
                        505,
                        523
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                246,
                                491,
                                505,
                                523
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        246,
                                        491,
                                        505,
                                        523
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\mathcal {L} = \\sum_ {t = 1} ^ {N} \\log \\mathcal {P} _ {\\theta} \\left(x _ {t + 1} \\mid x _ {1: t}\\right) \\tag {1}",
                                    "image_path": "8b7394247c847c5872ee73a8b2263a80bc8280ad6c9300f6fdeb4d71924ef51c.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        525,
                        504,
                        559
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                525,
                                504,
                                559
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "text",
                                    "content": "where "
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "text",
                                    "content": " represents the large language model and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\theta"
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "text",
                                    "content": " is the trainable parameters of the model, with the training objective to maximize the probability of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "inline_equation",
                                    "content": "x_{t + 1}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "text",
                                    "content": " as the next token, given the previous tokens "
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "inline_equation",
                                    "content": "x_{1:t} = x_1,\\ldots ,x_t"
                                },
                                {
                                    "bbox": [
                                        104,
                                        525,
                                        504,
                                        559
                                    ],
                                    "type": "text",
                                    "content": "."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        104,
                        567,
                        504,
                        590
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                567,
                                504,
                                590
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        567,
                                        504,
                                        590
                                    ],
                                    "type": "text",
                                    "content": "Language Multi-modal Models (LMMs). LMMs extend pre-trained LLMs to generate responses conditioned on input images. This is achieved by using visual tokens as a prefix:"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        239,
                        593,
                        505,
                        626
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                239,
                                593,
                                505,
                                626
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        239,
                                        593,
                                        505,
                                        626
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\mathcal {L} = \\sum_ {t = 1} ^ {N} \\log \\mathcal {P} _ {\\theta} \\left(x _ {t + 1} \\mid x _ {1: t}, \\mathbf {X}\\right) \\tag {2}",
                                    "image_path": "e42b038f386ec7577c9e41b48bf91f6776e9802ac04220ee3f3757f6d419d089.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        104,
                        628,
                        504,
                        651
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                628,
                                504,
                                651
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "text",
                                    "content": "where "
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X} \\in \\mathbb{R}^{l \\times c}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "text",
                                    "content": " represents the sequence of visual tokens [43, 51, 4], with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "inline_equation",
                                    "content": "l"
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "text",
                                    "content": " being the sequence length and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "inline_equation",
                                    "content": "c"
                                },
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        651
                                    ],
                                    "type": "text",
                                    "content": " the hidden dimension of the LLM."
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        104,
                        660,
                        504,
                        704
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                660,
                                504,
                                704
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "text",
                                    "content": "Image Tokenization. Previous works [45, 43, 51] widely explored how to encode input images into visual tokens. The tokenization schemes usually leverage a vision-language pre-trained image encoder "
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{F}^v"
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "text",
                                    "content": ", e.g. CLIP [61], to extract image features "
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{f}^{\\mathbf{v}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "text",
                                    "content": " from an input image "
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{I}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "text",
                                    "content": ". Then, the image features are converted into visual tokens using a connection module "
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{M}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        660,
                                        504,
                                        704
                                    ],
                                    "type": "text",
                                    "content": " as follows:"
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        247,
                        708,
                        505,
                        721
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                247,
                                708,
                                505,
                                721
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        247,
                                        708,
                                        505,
                                        721
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\mathbf {X} = \\mathcal {M} \\left(\\mathbf {f} ^ {\\mathbf {v}}\\right); \\quad \\mathbf {f} ^ {\\mathbf {v}} = \\mathcal {F} ^ {v} (\\mathbf {I}) \\tag {3}",
                                    "image_path": "1ec3249cf97f3b950068445ce5ccc6f8912203231e2ed5b621ea0b83ed577a6f.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 13
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "4"
                                }
                            ]
                        }
                    ],
                    "index": 14
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 3
        },
        {
            "para_blocks": [
                {
                    "type": "code",
                    "bbox": [
                        111,
                        89,
                        416,
                        198
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                111,
                                89,
                                416,
                                198
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        89,
                                        416,
                                        198
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\# \\mathbf{H}_0"
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "text",
                                            "content": " : Input embeddings for LLM (Original inputs args for traditional LMM); #vis_pos: the location of visual tokens; #X, "
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\mathbf{X}^{\\mathrm{stack}}"
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "text",
                                            "content": " : Original visual tokens, Extra high-resolution visual token list; # "
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "inline_equation",
                                            "content": "l_{start}"
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "text",
                                            "content": " n: Index of starting layer, and layer interval for stacking.   \n1 def forward(Ho, Xstack, "
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "inline_equation",
                                            "content": "l_{start}"
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "text",
                                            "content": " , n, vis_pos):   \n2 H = H0   \n3 for (idx, TransformerLayer) in enumerate(self.layers): # DeepStack: if idx >= "
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "inline_equation",
                                            "content": "l_{start}\\& (idx - l_{start})\\% n == 0"
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "text",
                                            "content": " .. "
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\begin{array}{r}\\bigstar\\mathbf{H}[vis\\_pos] + = \\mathbf{X}^{\\mathrm{stack}}[(idx - l_{start}) / / n] \\end{array}"
                                        },
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                416,
                                                198
                                            ],
                                            "type": "text",
                                            "content": "   \n4   \n5   \n6 # Original Transformer: H = TransformerLayer(H)"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "code_body"
                        },
                        {
                            "bbox": [
                                112,
                                75,
                                304,
                                87
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        112,
                                        75,
                                        304,
                                        87
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                112,
                                                75,
                                                304,
                                                87
                                            ],
                                            "type": "text",
                                            "content": "Algorithm 1: DeepStack PyTorch pseudocode."
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "code_caption"
                        }
                    ],
                    "index": 1,
                    "sub_type": "code",
                    "guess_lang": "latex"
                },
                {
                    "bbox": [
                        104,
                        213,
                        506,
                        312
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                213,
                                506,
                                312
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "text",
                                    "content": "The connection module "
                                },
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{M}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "text",
                                    "content": " can take various forms, mainly divided into projection modules [51, 49] and perceiver resamplers [4, 43]. In the former, "
                                },
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{M}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "text",
                                    "content": " is implemented as either a single-layer linear projection [51] or a multi-layer MLP [49], directly projecting dense image features into the hidden space of the LLM. In the latter, "
                                },
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{M}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        213,
                                        506,
                                        312
                                    ],
                                    "type": "text",
                                    "content": " utilizes a cross-attention mechanism with a set of fixed-length learnable queries to extract image features, similar to the approach in [7]. They transform dense image features into sparse image queries, which are then used as input tokens for the language model. However, the resamplers-based methods easily struggle with hallucinations on spatial reasoning tasks [17]. In this paper, we mainly focus on the projection-based connection module for its efficiency and effectiveness."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        105,
                        324,
                        320,
                        336
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                324,
                                320,
                                336
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        324,
                                        320,
                                        336
                                    ],
                                    "type": "text",
                                    "content": "3.2 DeepStack for Improved Image Tokenization"
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        104,
                        344,
                        504,
                        378
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                344,
                                504,
                                378
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        344,
                                        504,
                                        378
                                    ],
                                    "type": "text",
                                    "content": "Now that we obtain the visual tokens for LMMs using a projection-based connection module, the following challenge is how to provide informative visual tokens while keeping the multi-modal processing effective."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        382,
                        506,
                        439
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                382,
                                506,
                                439
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        382,
                                        506,
                                        439
                                    ],
                                    "type": "text",
                                    "content": "Scaling Visual Tokens. Based on the projection-based connection module, many follow-up attempts to increase the visual capability by introducing multiple image crops [50, 73] for scaling up the resolution or involving multiple vision encoders to serve as a mixture of visual experts [89, 73, 21]. For these approaches, the visual tokens from different image crops or vision encoders are concatenated together along the axis of the sequence or the dimension before projection."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        445,
                        506,
                        544
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                445,
                                506,
                                544
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": "DeepStack Strategy. In order to incorporate fine-grained image information while maintaining efficiency, we enhance the input visual tokens "
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": " by stacking high-resolution visual tokens into different LLM decoder layers. In practice, we first upsample the input image according to its aspect ratio and simultaneously tokenize it to obtain high-resolution visual tokens. To prepare the tokens for hierarchy stacking, we split the high-resolution visual tokens into different token sets "
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X}^{\\mathrm{stack}^i}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": " with spatial dilation [80, 14]. This sampling approach ensures that the visual tokens "
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X}^{\\mathrm{stack}^i}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": " have the same length as the global visual tokens "
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": ". Additionally, token "
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X}^{\\mathrm{stack}^i}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": " corresponds to the nearest neighbor of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{X}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        445,
                                        506,
                                        544
                                    ],
                                    "type": "text",
                                    "content": " in spatial."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        214,
                        545,
                        393,
                        578
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                214,
                                545,
                                393,
                                578
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        214,
                                        545,
                                        393,
                                        578
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\begin{array}{l} \\mathbf {X} ^ {\\text {s t a c k}} = \\left\\{\\mathbf {X} ^ {\\text {s t a c k} ^ {1}}, \\mathbf {X} ^ {\\text {s t a c k} ^ {2}}, \\dots , \\mathbf {X} ^ {\\text {s t a c k} ^ {\\mathbf {S}}} \\right\\} \\\\ = \\text {S a m p l i n g 2 D} \\left(\\mathcal {M} \\left(\\mathcal {F} ^ {v} \\left(\\mathbf {I} ^ {\\text {h i r e s}}\\right)\\right)\\right) \\\\ \\end{array}",
                                    "image_path": "040fce191d7462812fa962cb6d4ec42cc274c334e10da20ec651a6a8b98252ee.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        587,
                        506,
                        670
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                587,
                                506,
                                670
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": "As shown in Fig. 2, given an LLM of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "L"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " decoder layers, the LLM is first split into different blocks. Specifically, DeepStack split the early layers of LLM "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " into a set of deepstack blocks "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{B}^V = \\{\\mathcal{P}^{V^1},\\mathcal{P}^{V^2},\\dots,\\mathcal{P}^{V^n}\\}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " for stacking visual tokens, and the later layers into a plain block "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}^{\\mathbb{L}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " for original prefix sequential modeling. We denote that each deepstack block "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}^{V^i}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " ends at the "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "N^{V^i}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": "-th layer of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": ", while the plain block "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}^{\\mathbb{L}}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " ends at the last layer. We use "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{H}^i"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " to represent the hidden states of visual tokens after the "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "i"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": "-th transformer decoder layer, with "
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{H}^L"
                                },
                                {
                                    "bbox": [
                                        104,
                                        587,
                                        506,
                                        670
                                    ],
                                    "type": "text",
                                    "content": " being the visual hidden states after the final decoder layer. Formally, the output of each block can be formulated as follows:"
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        238,
                        673,
                        504,
                        725
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                238,
                                673,
                                504,
                                725
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        238,
                                        673,
                                        504,
                                        725
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\begin{array}{l} \\mathbf {H} ^ {V ^ {1}} = \\mathcal {P} ^ {V ^ {1}} (\\mathbf {X}) + \\mathbf {X} ^ {\\text {s t a c k} ^ {1}} \\\\ \\mathbf {H} ^ {V ^ {2}} = \\mathcal {P} ^ {V ^ {2}} \\left(\\mathbf {H} ^ {V ^ {1}}\\right) + \\mathbf {X} ^ {\\text {s t a c k} ^ {2}} \\tag {5} \\\\ \\mathbf {H} ^ {L} = \\mathcal {P} ^ {\\mathbb {L}} \\left(\\mathbf {H} ^ {V ^ {n}}\\right) \\\\ \\end{array}",
                                    "image_path": "4086ada1137ef530405f92b34eb2439572559c806a2765d085ab856a770e2e5b.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        308,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                308,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        308,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "5"
                                }
                            ]
                        }
                    ],
                    "index": 10
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 4
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        104,
                        72,
                        504,
                        95
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                72,
                                504,
                                95
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        95
                                    ],
                                    "type": "text",
                                    "content": "Specifically, we divide the layers into equally sized deepstack blocks, with the block length of 1 by default."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        103,
                        506,
                        170
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                103,
                                506,
                                170
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "text",
                                    "content": "DeepStack for Vision Transformers (ViTs). Our DeepStack can be also applied to ViTs for better feature extraction and image tokenization as illustrated in Fig. 2 (DeepStack-V). In contrast to LMM, we use the patch embedding layers PatchEmbedding and the first several ViT encoder layers for tokenization and the reset ViT encoder layers for DeepStack. Formally, we replace the "
                                },
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{F}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "text",
                                    "content": " and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{M}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "text",
                                    "content": " in Eq. (4) with the Patch Embedding Layers and the first several encoder layers, and utilize the rest of encoders layers as "
                                },
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathcal{P}"
                                },
                                {
                                    "bbox": [
                                        104,
                                        103,
                                        506,
                                        170
                                    ],
                                    "type": "text",
                                    "content": " in Eq. (5). Please refer to Sec. 4.3 for more details."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        178,
                        504,
                        233
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                178,
                                504,
                                233
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        178,
                                        504,
                                        233
                                    ],
                                    "type": "text",
                                    "content": "Comparison with Other Visual Token Enhancement Strategies. To provide a deeper understanding of the DeepStack mechanism, we compare our strategy with previous visual token enhancement strategies by examining the hidden states of visual tokens after the final LLM decoder layer, denoted as "
                                },
                                {
                                    "bbox": [
                                        104,
                                        178,
                                        504,
                                        233
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{H}^L"
                                },
                                {
                                    "bbox": [
                                        104,
                                        178,
                                        504,
                                        233
                                    ],
                                    "type": "text",
                                    "content": ". Previous methods can be broadly categorized into two approaches: Sequence Concatenation and Dimension Concatenation."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        238,
                        505,
                        272
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                238,
                                505,
                                272
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        238,
                                        505,
                                        272
                                    ],
                                    "type": "text",
                                    "content": "As for the former, visual tokens from the entire image and local crops are concatenated sequentially, significantly increasing the overall sequence length the computation cost. The LLM decoder processes these concatenated visual tokens as a longer visual prefix, directly modeling the extended sequence."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        241,
                        284,
                        505,
                        300
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                241,
                                284,
                                505,
                                300
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        241,
                                        284,
                                        505,
                                        300
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\mathbf {H} ^ {L} = \\mathcal {P} (\\operatorname {S e q C a t} [ \\mathbf {X}, \\mathbf {X} ^ {\\mathbf {s t a c k}} ]) \\tag {6}",
                                    "image_path": "492df6ac0e652706c6a69a6a92fabbf654e57dcc3e3906e6de2aa05bb6ebd154.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        312,
                        504,
                        345
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                312,
                                504,
                                345
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        312,
                                        504,
                                        345
                                    ],
                                    "type": "text",
                                    "content": "As for the latter, visual tokens are concatenated along the feature dimension, keeping the sequence length constant. When using a projection module as the connection module, the enhanced visual tokens can be viewed as the sum of features from two individual projection modules."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        233,
                        357,
                        505,
                        388
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                233,
                                357,
                                505,
                                388
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        233,
                                        357,
                                        505,
                                        388
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\begin{array}{l} \\begin{array}{l} \\mathbf {H} ^ {L} = \\mathcal {P} \\left(\\mathcal {M} (\\text {D i m C a t} [ \\mathbf {f}, \\mathbf {f} ^ {\\text {h i r e s}} ])\\right) \\\\ \\mathcal {P} (1 + 1 (x), \\dots , 1 + 3 (x h i r e s)) \\end{array} \\tag {7} \\\\ \\approx \\mathcal {P} \\left(\\mathcal {M} ^ {1} (\\mathbf {f}) + \\mathcal {M} ^ {2} \\left(\\mathbf {f} ^ {\\mathrm {h i r e s}}\\right)\\right) \\\\ \\end{array}",
                                    "image_path": "ffce432abfb94f9328bc51a3e571fdb3cadccbac5090ddf9aeea098160a5956b.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        400,
                        504,
                        488
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                400,
                                504,
                                488
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        400,
                                        504,
                                        488
                                    ],
                                    "type": "text",
                                    "content": "In our DeepStack, we employ a unique approach where enhancement occurs from bottom to top layer by layer. The processing of "
                                },
                                {
                                    "bbox": [
                                        104,
                                        400,
                                        504,
                                        488
                                    ],
                                    "type": "inline_equation",
                                    "content": "\\mathbf{H}^L"
                                },
                                {
                                    "bbox": [
                                        104,
                                        400,
                                        504,
                                        488
                                    ],
                                    "type": "text",
                                    "content": " in DeepStack unfolds in two phases. In the early layers of the decoder, the layers function similarly to an encoder, recurrently enhancing the input visual tokens by adding high-resolution visual tokens residually; In the later layers, the decoder performs plain sequence modeling as usual. This dual-phase processing fully leverages the LLM's capabilities by combining both encoding and sequence modeling. By integrating high-resolution visual information at multiple layers, DeepStack effectively enhances visual token representation without increasing visual context length, demonstrating its superiority over previous methods."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        191,
                        495,
                        367,
                        506
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                191,
                                495,
                                367,
                                506
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        191,
                                        495,
                                        367,
                                        506
                                    ],
                                    "type": "text",
                                    "content": "Deep layers for LLM sequence modeling"
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        149,
                        506,
                        505,
                        541
                    ],
                    "type": "interline_equation",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                149,
                                506,
                                505,
                                541
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        149,
                                        506,
                                        505,
                                        541
                                    ],
                                    "type": "interline_equation",
                                    "content": "\\mathbf {H} ^ {L} = \\overbrace {\\mathcal {P} ^ {\\mathbb {L}}} ^ {\\downarrow} \\left(\\overbrace {\\mathcal {P} ^ {V ^ {n}} \\left(\\dots \\left(\\mathcal {P} ^ {V 1} \\left(\\mathbf {X} + \\mathbf {X} ^ {\\mathbf {s t a c k} ^ {1}}\\right) + \\mathbf {X} ^ {\\mathbf {s t a c k} ^ {2}}\\right) \\dots\\right) + \\mathbf {X} ^ {\\mathbf {s t a c k} ^ {n}}}\\right)\\left. \\right. \\tag {8}",
                                    "image_path": "93257fe81d148be3bbb5ffedb59ddedacbb5f64595f4f1a19f02cc0160fba103.jpg"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        247,
                        542,
                        419,
                        554
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                247,
                                542,
                                419,
                                554
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        247,
                                        542,
                                        419,
                                        554
                                    ],
                                    "type": "text",
                                    "content": "Early layers for visual tokens encoding"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        105,
                        579,
                        192,
                        594
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                579,
                                192,
                                594
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        579,
                                        192,
                                        594
                                    ],
                                    "type": "text",
                                    "content": "4 Experiments"
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "bbox": [
                        105,
                        604,
                        231,
                        616
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                604,
                                231,
                                616
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        604,
                                        231,
                                        616
                                    ],
                                    "type": "text",
                                    "content": "4.1 Implementation Details"
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        104,
                        625,
                        504,
                        681
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                625,
                                504,
                                681
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        625,
                                        504,
                                        681
                                    ],
                                    "type": "text",
                                    "content": "We mainly follow the training recipe of Llava [51], of which the training pipeline consists of two stages, i.e. pre-training (PT) stage and supervised-finetuning (SFT) stage. We utilize pre-trained CLIP-large-336 [61] as our default image encoder. To obtain high-resolution feature maps, we split the high-resolution image into patches to comply with the resolution requirement and mosaic the image feature together as whole-image features."
                                }
                            ]
                        }
                    ],
                    "index": 13
                },
                {
                    "bbox": [
                        104,
                        689,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                689,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        689,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "Pre-training dataset. We utilize LCS-558k [51] as pre-training data for both experiments based on LLaVA-1.5 and LLaVA-Next, which contain 558k samples from LAION [66], CC [9] and SBU [84], captioned by BLIP [45]."
                                }
                            ]
                        }
                    ],
                    "index": 14
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        742,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                742,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        742,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "6"
                                }
                            ]
                        }
                    ],
                    "index": 15
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 5
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        107,
                        70,
                        504,
                        240
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                107,
                                70,
                                504,
                                240
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        70,
                                        504,
                                        240
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                70,
                                                504,
                                                240
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">LLM</td><td rowspan=\"2\">Eff. Res.</td><td rowspan=\"2\">Vis. Tok.</td><td rowspan=\"2\">Cxt. Len.</td><td rowspan=\"2\">PT</td><td rowspan=\"2\">SFT</td><td colspan=\"2\">General VQA</td><td colspan=\"3\">Text-oriented VQA</td><td colspan=\"4\">LMM benchmarks</td></tr><tr><td>VQAV2</td><td>GQA</td><td>Text VQA‡</td><td>Doc VQA‡</td><td>Info VQA‡</td><td>SEED (all)</td><td>POPE (all)</td><td>MM MU‡</td><td>MM Vet</td></tr><tr><td>BLIP-2 [43]</td><td>Vicuna-13B</td><td>224</td><td>32</td><td>32</td><td>129M</td><td>-</td><td>41.0</td><td>41.0</td><td>42.5</td><td></td><td></td><td>46.4</td><td>85.3</td><td>-</td><td></td></tr><tr><td>InstructBLIP [16]</td><td>Vicuna-7B</td><td>224</td><td>32</td><td>32</td><td>129M</td><td>1.2M</td><td>-</td><td>49.2</td><td>50.1</td><td>-</td><td>-</td><td>53.4</td><td>-</td><td>-</td><td>-</td></tr><tr><td>InstructBLIP [16]</td><td>Vicuna-13B</td><td>224</td><td>32</td><td>32</td><td>129M</td><td>1.2M</td><td>-</td><td>49.5</td><td>50.7</td><td>-</td><td>-</td><td>78.9</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Shikra [12]</td><td>Vicuna-13B</td><td>224</td><td>-</td><td>-</td><td>600K</td><td>5.5M</td><td>77.4*</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>IDEFICS-9B [37]</td><td>LLaMA-7B</td><td>224</td><td>-</td><td>-</td><td>353M</td><td>1M</td><td></td><td>50.9</td><td>38.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>IDEFICS-80B [37]</td><td>LLaMA-65B</td><td>224</td><td>-</td><td>-</td><td>353M</td><td>1M</td><td>60.0</td><td>45.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Qwen-VL [5]</td><td>Qwen-7B</td><td>448</td><td>256</td><td>256</td><td>1.4B</td><td>50M</td><td>78.8*</td><td>59.3*</td><td>63.8</td><td>-</td><td>-</td><td>56.3</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Qwen-VL-Chat [5]</td><td>Qwen-7B</td><td>448</td><td>256</td><td>256</td><td>1.4B</td><td>50M</td><td>78.2*</td><td>57.5*</td><td>61.5</td><td>-</td><td>-</td><td>58.2</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VILA [47]</td><td>Llama2-7B</td><td>336</td><td>576</td><td>576</td><td>50M</td><td>1M</td><td>79.9*</td><td>62.3*</td><td>64.4</td><td>-</td><td>-</td><td>61.1</td><td>85.5</td><td>-</td><td>34.9</td></tr><tr><td>VILA [47]</td><td>Llama2-13B</td><td>336</td><td>576</td><td>576</td><td>50M</td><td>1M</td><td>80.8</td><td>63.3*</td><td>66.6</td><td>-</td><td>-</td><td>62.8</td><td>84.2</td><td>-</td><td>38.8</td></tr><tr><td>LLaVA-1.5 [49]</td><td>Vicuna-7B</td><td>336</td><td>576</td><td>576</td><td>558K</td><td>665K</td><td>78.5*</td><td>62.0*</td><td>58.2</td><td>28.1</td><td>25.8</td><td>58.6</td><td>85.9</td><td>35.3</td><td>30.5</td></tr><tr><td>LLaVA-1.5 [49]</td><td>Vicuna-13B</td><td>672</td><td>576</td><td>576</td><td>558K</td><td>665K</td><td>80.0*</td><td>63.3*</td><td>61.3</td><td>30.3</td><td>28.4</td><td>61.6</td><td>85.9</td><td>34.8</td><td>35.4</td></tr><tr><td>LLaVA-Next [50]</td><td>Vicuna-7B</td><td>672</td><td>2880</td><td>2880</td><td>558K</td><td>765K</td><td>81.8*</td><td>64.2*</td><td>64.9</td><td>74.4*</td><td>37.1*</td><td>64.7</td><td>86.5</td><td>35.1</td><td>44.1</td></tr><tr><td>LLaVA-Next [50]</td><td>Vicuna-7B</td><td>672</td><td>2880</td><td>2880</td><td>558K</td><td>765K</td><td>82.8*</td><td>65.4*</td><td>66.9</td><td>77.5*</td><td>44.5*</td><td>65.6</td><td>86.2</td><td>35.9</td><td>49.1</td></tr><tr><td>DeepStack-V</td><td>Vicuna-7B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>80.4*</td><td>64.1*</td><td>63.5</td><td>41.0</td><td>30.0</td><td>62.3</td><td>87.6</td><td>34.9</td><td>33.0</td></tr><tr><td>DeepStack-V</td><td>Vicuna-13B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>81.1</td><td>64.2*</td><td>63.9</td><td>41.7</td><td>33.1</td><td>63.0</td><td>86.6</td><td>34.7</td><td>31.1</td></tr><tr><td>DeepStack-L</td><td>Vicuna-7B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>79.5*</td><td>63.1*</td><td>62.4</td><td>39.1</td><td>29.8</td><td>60.6</td><td>86.7</td><td>35.7</td><td>29.9</td></tr><tr><td>DeepStack-L</td><td>Vicuna-13B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>80.9*</td><td>64.2*</td><td>64.6</td><td>41.5</td><td>33.0</td><td>63.5</td><td>87.7</td><td>35.2</td><td>35.9</td></tr><tr><td>DeepStack-L-HD†</td><td>Vicuna-7B</td><td>1344</td><td>14400</td><td>2880</td><td>558K</td><td>748K</td><td>82.0*</td><td>65.2*</td><td>66.7</td><td>78.8*</td><td>41.2*</td><td>63.6</td><td>86.5</td><td>35.6</td><td>37.5</td></tr><tr><td>DeepStack-L-HD†</td><td>Vicuna-13B</td><td>1344</td><td>14400</td><td>2880</td><td>558K</td><td>748K</td><td>83.0*</td><td>66.2*</td><td>68.7</td><td>81.0*</td><td>45.2*</td><td>65.1</td><td>86.7</td><td>33.4</td><td>39.3</td></tr></table>",
                                            "image_path": "28cf535e5082bca39a50a3131d27dc3c95491f90fa2fac33539003c9c5e5976b.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                240,
                                506,
                                331
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        240,
                                        506,
                                        331
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                240,
                                                506,
                                                331
                                            ],
                                            "type": "text",
                                            "content": "Table 1: Comparison with other LMMs on 9 benchmarks. Eff. Res. indicates the effective image resolution taken by each method. Vis. Tok. indicates the number of visual tokens used for LLMs (not only for the input layers); Cxt. Len. indicates the input visual context length of LLMs. Previous methods feed the visual tokens as the input embeddings, thus the Vis. Tok. = Cxt. Len. all the time. For comparison with LLaVA-Next, since 765K instruction tuning data is not available, our model is fine-tuned on our 748K data. † indicates that our model is fine-tuned from LLaVA-Next. * The training images of the datasets are observed during training. ‡ denotes we report the performance on validation sets. We unfreeze the vision encoder in DeepStack-V and DeepStack-L-HD while freezing it in DeepStack-L for a fair comparison with previous methods. We fine-tuning the vision encoder can bring further improvement on DeepStack-L (please refer to Sec. 4.3 and Supp.)"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_footnote"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        344,
                        506,
                        389
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                344,
                                506,
                                389
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        344,
                                        506,
                                        389
                                    ],
                                    "type": "text",
                                    "content": "Fine-tuning datasets. We utilize LLaVA-mixed-665k [51] as instruction-following data for both experiments based on LLaVA-1.5. However, the SFT dataset used in Llava-Next is not publicly available, we thus combine an SFT dataset of 748K samples following the guidance [50]. In contrast, we do not involve the user images uploaded to their website."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        396,
                        506,
                        464
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                396,
                                506,
                                464
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        396,
                                        506,
                                        464
                                    ],
                                    "type": "text",
                                    "content": "Training configuration. We train our model with only the projection model tuned in the PT stage. In SFT stage, we unfreeze LLM. For Experiments on DeepStack-V and DeepStack-HD, we tune the image encoder with a learning rate of 1e-6 following [50]. Otherwise, we freeze our vision encoder for a fair comparison. We use "
                                },
                                {
                                    "bbox": [
                                        104,
                                        396,
                                        506,
                                        464
                                    ],
                                    "type": "inline_equation",
                                    "content": "16 \\times"
                                },
                                {
                                    "bbox": [
                                        104,
                                        396,
                                        506,
                                        464
                                    ],
                                    "type": "text",
                                    "content": " V100 for experiments with Phi-3 [1] and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        396,
                                        506,
                                        464
                                    ],
                                    "type": "inline_equation",
                                    "content": "8 \\times"
                                },
                                {
                                    "bbox": [
                                        104,
                                        396,
                                        506,
                                        464
                                    ],
                                    "type": "text",
                                    "content": " H100 for experiments with Vicuna [15]. Please refer to our supplementary material for more detailed training hyper-parameters."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        105,
                        475,
                        211,
                        488
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                475,
                                211,
                                488
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        475,
                                        211,
                                        488
                                    ],
                                    "type": "text",
                                    "content": "4.2 Quantitative Results"
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        495,
                        507,
                        563
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                495,
                                507,
                                563
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        495,
                                        507,
                                        563
                                    ],
                                    "type": "text",
                                    "content": "We evaluate DeepStack on a range of benchmarks, encompassing both academic task-oriented evaluations and recent large multi-modal language model (LMM) benchmarks. Specifically, we focus on text-oriented datasets, including ChartVQA [54], DocVQA [56], InfoVQA [55], MultiDocVQA [72], TextVQA [69], to demonstrate effectiveness in high-resolution scenarios. Additionally, we perform zero-shot evaluations of DeepStack on commonly used video understanding benchmarks to assess its performance on finer-grained tasks."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        570,
                        506,
                        681
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                570,
                                506,
                                681
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        570,
                                        506,
                                        681
                                    ],
                                    "type": "text",
                                    "content": "General VQA and LMM benchmarks. We assess DeepStack on two classic general VQA benchmarks, VQAv2 [23] and GQA [25], as well as five recent LMM benchmarks: SEED [40], POPE [46], MMMU [83], and MM-Vet [81]. As presented in Tab. 1, DeepStack outperforms its direct baseline model, LLaVA, on both VQAv2 and GQA, showcasing state-of-the-art performance in traditional VQA tasks. Furthermore, DeepStack consistently surpasses other methods on the recent LMM benchmarks. DeepStack achieves comparable performance on MM-Vet on the experiments based on LLaVA-1.5. However, due to we lack of fancy instruction-following data used in LLaVA-mix-765K, our experiments with LLaVA-Next lag behind the LLaVA-Next. Notably, the significant performance boost on the POPE benchmark suggests that our DeepStack strategy effectively alleviates visual hallucination by providing rich and detailed visual information for visual understanding."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        689,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                689,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        689,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "Text-Oriented benchmarks. To further validate the effectiveness of DeepStack, we evaluate it on more text-oriented benchmarks, including ChartQA [54], DocVQA [56], InfoVQA [55], MultiDocVQA [72], and TextVQA [69]. These benchmarks contain high-resolution images and typically"
                                }
                            ]
                        }
                    ],
                    "index": 7
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        309,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                309,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        309,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "7"
                                }
                            ]
                        }
                    ],
                    "index": 8
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 6
        },
        {
            "para_blocks": [
                {
                    "type": "table",
                    "bbox": [
                        116,
                        70,
                        492,
                        183
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                116,
                                70,
                                492,
                                183
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        116,
                                        70,
                                        492,
                                        183
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                116,
                                                70,
                                                492,
                                                183
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Method</td><td>LLM</td><td>Vis. Tok.</td><td>Cxt. Len.</td><td>PT</td><td>IT</td><td>Chart QA‡</td><td>Doc VQA‡</td><td>Info VQA‡</td><td>MultiDoc VQA‡</td><td>Text VQA‡</td></tr><tr><td>LLaVA-1.5 [49]</td><td>Vicuna-7B</td><td>576</td><td>576</td><td>558K</td><td>665K</td><td>18.2</td><td>28.1</td><td>25.8</td><td>16.7 / 7.2</td><td>58.2*</td></tr><tr><td>LLaVA-1.5 [49]</td><td>Vicuna-13B</td><td>576</td><td>576</td><td>558K</td><td>665K</td><td>18.2</td><td>30.3</td><td>29.4</td><td>18.3 / 8.0</td><td>61.2*</td></tr><tr><td>LLaVA-Next [50]</td><td>Vicuna-7B</td><td>2880</td><td>2880</td><td>558K</td><td>765K</td><td>54.8</td><td>74.4</td><td>37.1</td><td>44.4 / 31.3</td><td>64.9</td></tr><tr><td>LLaVA-Next [50]</td><td>Vicuna-13B</td><td>2880</td><td>2880</td><td>558K</td><td>765K</td><td>62.2</td><td>77.5</td><td>44.5</td><td>46.3 / 32.6</td><td>66.9</td></tr><tr><td>DeepStack-V</td><td>Vicuna-7B</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>20.6</td><td>41.0</td><td>30.0</td><td>23.0 / 11.0</td><td>63.5*</td></tr><tr><td>DeepStack-V</td><td>Vicuna-13B</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>20.2</td><td>41.7</td><td>33.1</td><td>23.5 / 11.2</td><td>63.9*</td></tr><tr><td>DeepStack-L</td><td>Vicuna-7B</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>21.0</td><td>39.3</td><td>30.1</td><td>22.2 / 10.5</td><td>64.5*</td></tr><tr><td>DeepStack-L</td><td>Vicuna-13B</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>21.2</td><td>43.1</td><td>34.0</td><td>24.8 / 12.2</td><td>65.2*</td></tr><tr><td>DeepStack-HD†</td><td>Vicuna-7B</td><td>14400</td><td>2880</td><td>558K</td><td>748K</td><td>56.3</td><td>78.8</td><td>41.2</td><td>48.2 / 37.7</td><td>66.7</td></tr><tr><td>DeepStack-HD†</td><td>Vicuna-13B</td><td>14400</td><td>2880</td><td>748K</td><td>748K</td><td>64.0</td><td>81.0</td><td>45.2</td><td>49.4 / 39.1</td><td>68.7</td></tr></table>",
                                            "image_path": "18b85c2037c986988f57afd33f415d464a21695f314b89a3534c0223f044185a.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                105,
                                184,
                                504,
                                205
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        105,
                                        184,
                                        504,
                                        205
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                105,
                                                184,
                                                504,
                                                205
                                            ],
                                            "type": "text",
                                            "content": "Table 2: Results on Text-Oriented benchmarks, where high resolution is essential. * denotes we use OCR tokens for TextVQA following LLaVA-1.5. ‡ denotes we report the performance on validation sets."
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        227,
                        506,
                        262
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                227,
                                506,
                                262
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        227,
                                        506,
                                        262
                                    ],
                                    "type": "text",
                                    "content": "require the model to answer questions based on fine-grained visual inputs. As shown in Tab. 2, equipping our model with DeepStack results in consistent gains across all benchmarks. This strongly demonstrates that DeepStack enhances visual token even without increasing sequence length."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        104,
                        269,
                        504,
                        335
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                269,
                                504,
                                335
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        269,
                                        504,
                                        335
                                    ],
                                    "type": "text",
                                    "content": "Zero-shot performance on Video QA benchmarks. We also conduct zero-shot evaluations on video QA benchmarks, including EgoSchema [52] and Next-QA [78] for multiple-choice VQA, and MSVD-QA [10, 79] and ActivityNet-QA [82] for open-ended VQA. Inspired by [33], we sample frames from each video uniformly and mosaic the frames into images to adapt video QA tasks to the image domain. Thanks to the higher effective resolution brought by refined visual tokens, DeepStack effectively handles zero-shot video QA tasks even without being fine-tuned on any video data."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "table",
                    "bbox": [
                        148,
                        350,
                        462,
                        407
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                148,
                                350,
                                462,
                                407
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        148,
                                        350,
                                        462,
                                        407
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                148,
                                                350,
                                                462,
                                                407
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td rowspan=\"3\">Method</td><td colspan=\"5\">Multi-choice VQA</td><td colspan=\"4\">Open-ended VQA</td></tr><tr><td rowspan=\"2\">EgoSchema</td><td rowspan=\"2\">Cas.</td><td colspan=\"2\">Next-QA</td><td rowspan=\"2\">Acc.</td><td colspan=\"2\">MSVD</td><td colspan=\"2\">ActivityNet</td></tr><tr><td>Des.</td><td>Tem.</td><td>Acc.</td><td>Score</td><td>Acc.</td><td>Score</td></tr><tr><td>LLaVA-1.5-7B</td><td>35.4</td><td>59.5</td><td>68.9</td><td>55.5</td><td>59.6</td><td>75.5</td><td>4.0</td><td>48.6</td><td>3.2</td></tr><tr><td>DeepStack-L-7B</td><td>38.4</td><td>61.9</td><td>69.4</td><td>55.5</td><td>61.0</td><td>76.0</td><td>4.0</td><td>49.3</td><td>3.1</td></tr></table>",
                                            "image_path": "555df7b32ffeaf5ef7fcdabd7786076c1c34d7bfdff1532f8793e2e8112459b2.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                407,
                                504,
                                440
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        407,
                                        504,
                                        440
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                407,
                                                504,
                                                440
                                            ],
                                            "type": "text",
                                            "content": "Table 3: Zero-shot evaluation on Video QA benchmarks. We collate 6 frames uniformly sampled from each video into "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                407,
                                                504,
                                                440
                                            ],
                                            "type": "inline_equation",
                                            "content": "2 \\times 3"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                407,
                                                504,
                                                440
                                            ],
                                            "type": "text",
                                            "content": " grid and resize the resulting image to saquare. Our model clearly outperforms the baseline because more visual information is included with the same context length. We mark the best performance bold."
                                        }
                                    ]
                                }
                            ],
                            "index": 5,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        477,
                        205,
                        490
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                477,
                                205,
                                490
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        477,
                                        205,
                                        490
                                    ],
                                    "type": "text",
                                    "content": "4.3 Model Inspection"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        499,
                        506,
                        565
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                499,
                                506,
                                565
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        499,
                                        506,
                                        565
                                    ],
                                    "type": "text",
                                    "content": "We further conduct sufficient experiments to give in-depth inspiration on the mechanism of DeepStack. In this section, we experiment with phi-3 [1] as the language backbone for the training efficiency. We report the performance on 7 benchmarks, including 1 general VQA (GQA), 2 multi-modal benchmarks (POPE and SEED), and 4 text-oriented VQA (TextVQA, DocVQA, ChartQA and InforVQA). We can evaluate the model performance by comparing the average scores over the 7 benchmarks."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "image",
                    "bbox": [
                        126,
                        594,
                        232,
                        670
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                126,
                                594,
                                232,
                                670
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        126,
                                        594,
                                        232,
                                        670
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                126,
                                                594,
                                                232,
                                                670
                                            ],
                                            "type": "image",
                                            "image_path": "854d217912eef4232196531190d6f57106510031a3f941e54c6f01b5182a29b0.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 9,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                133,
                                581,
                                250,
                                591
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        133,
                                        581,
                                        250,
                                        591
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                133,
                                                581,
                                                250,
                                                591
                                            ],
                                            "type": "text",
                                            "content": "(a) Starting layer to insert visual tokens"
                                        }
                                    ]
                                }
                            ],
                            "index": 8,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 9
                },
                {
                    "type": "image",
                    "bbox": [
                        247,
                        594,
                        353,
                        669
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                247,
                                594,
                                353,
                                669
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        247,
                                        594,
                                        353,
                                        669
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                247,
                                                594,
                                                353,
                                                669
                                            ],
                                            "type": "image",
                                            "image_path": "cfde4bfd1371e178de0f43fd4ef0d0fbcc9aa559dbc212b20dbabc4a1be2997a.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 11,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                104,
                                678,
                                504,
                                719
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        678,
                                        504,
                                        719
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                678,
                                                504,
                                                719
                                            ],
                                            "type": "text",
                                            "content": "Figure 3: Analysis on using LLM layers to process visual tokens. (a) We insert the visual tokens into different starting layers and initialize the correspondence input embeddings as zero; (b) We fix the first layer to insert global visual tokens and ablation on the interval "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                678,
                                                504,
                                                719
                                            ],
                                            "type": "inline_equation",
                                            "content": "s"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                678,
                                                504,
                                                719
                                            ],
                                            "type": "text",
                                            "content": " for stacking high-resolution tokens; (c) We ablation number of layers for token stacking."
                                        }
                                    ]
                                }
                            ],
                            "index": 14,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                257,
                                582,
                                353,
                                591
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        257,
                                        582,
                                        353,
                                        591
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                257,
                                                582,
                                                353,
                                                591
                                            ],
                                            "type": "text",
                                            "content": "(b) Layer interval for DeepStack"
                                        }
                                    ]
                                }
                            ],
                            "index": 10,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "image",
                    "bbox": [
                        368,
                        594,
                        474,
                        669
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                368,
                                594,
                                474,
                                669
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        368,
                                        594,
                                        474,
                                        669
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                368,
                                                594,
                                                474,
                                                669
                                            ],
                                            "type": "image",
                                            "image_path": "73bfca95c64923a3ad4957495e1eba6d8746f1fb211452779d24aa362bd59372.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 13,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                374,
                                582,
                                481,
                                591
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        374,
                                        582,
                                        481,
                                        591
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                374,
                                                582,
                                                481,
                                                591
                                            ],
                                            "type": "text",
                                            "content": "(c) Number of layers for DeepStack"
                                        }
                                    ]
                                }
                            ],
                            "index": 12,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 13
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        308,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                308,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        308,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "8"
                                }
                            ]
                        }
                    ],
                    "index": 15
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 7
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        104,
                        72,
                        506,
                        216
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                72,
                                506,
                                216
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        506,
                                        216
                                    ],
                                    "type": "text",
                                    "content": "LLMs can well process visual tokens in the early decoder layers. To understand why earlier layers of LLMs are suitable for processing visual tokens, we conducted an experiment on the insertion layer for visual tokens. Traditionally, visual tokens are inserted at the input layer, e.g. 0-th layer. We progressively insert them deeper, initializing the corresponding input embeddings to zero. As shown in Fig. 3 (a), inserting visual tokens before the 8th of 32 decoder layers in Phi-3 results in acceptable performance variations. However, inserting them beyond the midpoint leads to a significant performance drop. This confirms that earlier layers efficiently handle initial visual information integration. We also explore the impact of inserting visual tokens at non-consecutive layers. In Fig. 3 (b), we fixed global visual tokens at the input layer and varied the interval between two decoder layers for stacking high-resolution tokens. All stacking settings consistently improved performance. Finally, we explored the number of layers used for stacking high-resolution tokens. As shown in Fig. 3 (c), increasing the layers for stacking consistently enhances overall performance, with the best results achieved using four layers."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        104,
                        223,
                        376,
                        310
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                223,
                                376,
                                310
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        223,
                                        376,
                                        310
                                    ],
                                    "type": "text",
                                    "content": "DeepStack can also boost Vision Transformers (ViT). To further explore the potential of DeepStack for vision transformers, we utilize the DeepStack on ViT. Specifically, we use the patch embedding layers and the first "
                                },
                                {
                                    "bbox": [
                                        104,
                                        223,
                                        376,
                                        310
                                    ],
                                    "type": "inline_equation",
                                    "content": "N"
                                },
                                {
                                    "bbox": [
                                        104,
                                        223,
                                        376,
                                        310
                                    ],
                                    "type": "text",
                                    "content": " ViT encoder layers to extract visual tokens, including the original tokens and "
                                },
                                {
                                    "bbox": [
                                        104,
                                        223,
                                        376,
                                        310
                                    ],
                                    "type": "inline_equation",
                                    "content": "4 \\times"
                                },
                                {
                                    "bbox": [
                                        104,
                                        223,
                                        376,
                                        310
                                    ],
                                    "type": "text",
                                    "content": " extra high-resolution tokens, and then stack the high-resolution tokens into the next 4 encoder layers, respectively. We need to unfreeze the vision encoder to adapt the pre-trained encoder to our DeepStack. As shown in Tab. 4"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        311,
                        504,
                        344
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                311,
                                504,
                                344
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        311,
                                        504,
                                        344
                                    ],
                                    "type": "text",
                                    "content": "and Sec. 4.3, when using the first 16 ViT encoder layers (total 24 layers for our ViT-Large) to extract visual tokens before DeepStack, DeepStack-V surpass the baseline model. And the performance keeps increasing when using more encoder layers before DeepStack."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "image",
                    "bbox": [
                        384,
                        224,
                        493,
                        308
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                384,
                                224,
                                493,
                                308
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        384,
                                        224,
                                        493,
                                        308
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                384,
                                                224,
                                                493,
                                                308
                                            ],
                                            "type": "image",
                                            "image_path": "7582e9d14b89d1813d910104cfe1068c001edf5843b68aa156d672099adf79e1.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 3,
                            "angle": 0,
                            "type": "image_body"
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "table",
                    "bbox": [
                        106,
                        361,
                        504,
                        450
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                361,
                                504,
                                450
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        361,
                                        504,
                                        450
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                361,
                                                504,
                                                450
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Tok. Enhance</td><td>N Layers before DeepStack</td><td>Ft Enc.</td><td>GQA</td><td>POPE</td><td>SEED</td><td>TextVQA</td><td>DocVQA</td><td>ChartQA</td><td>InfoVQA</td><td>AVG</td></tr><tr><td>None</td><td>None</td><td></td><td>62.5</td><td>85.5</td><td>63.5</td><td>56.7</td><td>31.7</td><td>15.8</td><td>28.3</td><td>49.1</td></tr><tr><td>None</td><td>None</td><td>✓</td><td>62.4</td><td>85.8</td><td>64.0</td><td>56.1</td><td>27.5</td><td>15.3</td><td>28.3</td><td>48.5</td></tr><tr><td>DeepStack-V</td><td>PatchEmbed+0 Enc. Layers</td><td>✓</td><td>56.9</td><td>80.8</td><td>54.9</td><td>44.4</td><td>13.7</td><td>12.3</td><td>25.3</td><td>41.2</td></tr><tr><td>DeepStack-V</td><td>PatchEmbed+4 Enc. Layers</td><td>✓</td><td>58.7</td><td>83.1</td><td>57.4</td><td>48.2</td><td>17.0</td><td>13.2</td><td>26.1</td><td>43.4</td></tr><tr><td>DeepStack-V</td><td>PatchEmbed+8 Enc. Layers</td><td>✓</td><td>60.4</td><td>84.2</td><td>59.7</td><td>51.8</td><td>23.1</td><td>14.7</td><td>26.6</td><td>45.8</td></tr><tr><td>DeepStack-V</td><td>PatchEmbed+12 Enc. Layers</td><td>✓</td><td>61.8</td><td>85.5</td><td>62.1</td><td>55.5</td><td>29.3</td><td>16.0</td><td>26.2</td><td>48.1</td></tr><tr><td>DeepStack-V</td><td>PatchEmbed+16 Enc. Layers</td><td>✓</td><td>62.9</td><td>86.3</td><td>63.9</td><td>59.1</td><td>36.9</td><td>18.2</td><td>29.3</td><td>50.9</td></tr><tr><td>DeepStack-V</td><td>PatchEmbed+20 Enc. Layers</td><td>✓</td><td>62.8</td><td>86.1</td><td>64.0</td><td>60.1</td><td>38.4</td><td>17.1</td><td>30.6</td><td>51.3</td></tr></table>",
                                            "image_path": "ee11aecb39fe642b33d9c424ad484275de69d5ad2667a90a6e9158c195072734.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                162,
                                450,
                                448,
                                460
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        162,
                                        450,
                                        448,
                                        460
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                162,
                                                450,
                                                448,
                                                460
                                            ],
                                            "type": "text",
                                            "content": "Table 4: Ablations on the number of ViT encoder layers for DeepStack-V."
                                        }
                                    ]
                                }
                            ],
                            "index": 5,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        104,
                        475,
                        357,
                        562
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                475,
                                357,
                                562
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        475,
                                        357,
                                        562
                                    ],
                                    "type": "text",
                                    "content": "Better spatial consistency leads to better performance. Different sampling strategies may lead to different results. In Tab. 5, we compare our default strategy with two other variants for organizing the visual tokens. As shown in Fig. 5, 2d Grid use each of the local crop as a layer and 1d Sequence simply flatten the visual tokens to one-dimensional and then reshape them into a layer stack. Accordingly, keeping the spatial coherence, i.e. 2d Spatial, as in our default setting could achieve"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "image",
                    "bbox": [
                        365,
                        477,
                        503,
                        525
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                365,
                                477,
                                503,
                                525
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        365,
                                        477,
                                        503,
                                        525
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                365,
                                                477,
                                                503,
                                                525
                                            ],
                                            "type": "image",
                                            "image_path": "c0d9434a8a8b414bd093b8f59d340fb322b0a1ff74da2d1ec4d30d8c1265fd6a.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 7,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                362,
                                529,
                                505,
                                550
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        362,
                                        529,
                                        505,
                                        550
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                362,
                                                529,
                                                505,
                                                550
                                            ],
                                            "type": "text",
                                            "content": "Figure 5: Visualization of three sampling methods for DeepStack."
                                        }
                                    ]
                                }
                            ],
                            "index": 8,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 7
                },
                {
                    "type": "table",
                    "bbox": [
                        124,
                        580,
                        487,
                        651
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                124,
                                580,
                                487,
                                651
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        124,
                                        580,
                                        487,
                                        651
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                124,
                                                580,
                                                487,
                                                651
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Consistent</td><td>Sampling</td><td>GQA</td><td>POPE</td><td>SEED</td><td>TextVQA</td><td>DocVQA</td><td>ChartQA</td><td>InfoVQA</td><td>AVG</td></tr><tr><td>None</td><td>None</td><td>62.5</td><td>85.5</td><td>63.5</td><td>56.7</td><td>31.7</td><td>15.8</td><td>28.3</td><td>49.1</td></tr><tr><td>X</td><td>2d Spatial</td><td>62.2</td><td>85.1</td><td>62.3</td><td>58.1</td><td>35.1</td><td>16.4</td><td>30.1</td><td>49.9</td></tr><tr><td>✓</td><td>2d Spatial</td><td>63.0</td><td>86.4</td><td>62.9</td><td>58.8</td><td>38.7</td><td>17.2</td><td>30.8</td><td>51.1</td></tr><tr><td>✓</td><td>2d Grid</td><td>60.6</td><td>86.2</td><td>61.2</td><td>57.1</td><td>33.2</td><td>16.4</td><td>28.6</td><td>49.0</td></tr><tr><td>✓</td><td>1d Sequential</td><td>61.6</td><td>86.2</td><td>61.9</td><td>57.1</td><td>33.1</td><td>15.2</td><td>30.0</td><td>49.3</td></tr></table>",
                                            "image_path": "bdba7a0ce7aed3dcf72bd9032b1d7805a5c4607332c7e7b532e08ee202638817.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 9,
                            "angle": 0,
                            "type": "table_body"
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        104,
                        651,
                        504,
                        712
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                651,
                                504,
                                712
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        651,
                                        504,
                                        712
                                    ],
                                    "type": "text",
                                    "content": "Table 5: Ablations on image consistency and sampling method. We apply the Resize transformation to both the original image and the high-resolution image for consistency. For inconsistency, we use Resize on the original image and Pad-Resize on the high-resolution image. 2d Spatial refers to sampling based on spatial locations, such as using a 4-neighbor method. 2d Grid means the visual tokens are divided into 2d grids, with each grid stacked per layer. 1d Sequential indicates that the high-resolution visual tokens are first flattened into a sequence and then uniformly sampled for each layer. Please refer to Fig. 5 for better understanding."
                                }
                            ]
                        }
                    ],
                    "index": 10
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        302,
                        741,
                        308,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                302,
                                741,
                                308,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        302,
                                        741,
                                        308,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "9"
                                }
                            ]
                        }
                    ],
                    "index": 11
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 8
        },
        {
            "para_blocks": [
                {
                    "type": "image",
                    "bbox": [
                        109,
                        72,
                        193,
                        134
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                109,
                                72,
                                193,
                                134
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        109,
                                        72,
                                        193,
                                        134
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                109,
                                                72,
                                                193,
                                                134
                                            ],
                                            "type": "image",
                                            "image_path": "3e3199eb08b40b447e9f6329a376117e8870f3201b9171befb4256eadcdacb2a.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 0,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                111,
                                137,
                                200,
                                151
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        137,
                                        200,
                                        151
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                137,
                                                200,
                                                151
                                            ],
                                            "type": "text",
                                            "content": "Q: What does it say in the bottom right corner?"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "image",
                    "bbox": [
                        211,
                        72,
                        304,
                        134
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                211,
                                72,
                                304,
                                134
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        211,
                                        72,
                                        304,
                                        134
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                211,
                                                72,
                                                304,
                                                134
                                            ],
                                            "type": "image",
                                            "image_path": "d132137cd0c95efde0ade4c4b77ce04708648a3c237a5f3515fefd9d42da8731.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 2,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                110,
                                163,
                                172,
                                170
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        110,
                                        163,
                                        172,
                                        170
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                110,
                                                163,
                                                172,
                                                170
                                            ],
                                            "type": "text",
                                            "content": "DeepStack: 'Postcode'"
                                        }
                                    ]
                                }
                            ],
                            "index": 12,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                211,
                                137,
                                304,
                                151
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        211,
                                        137,
                                        304,
                                        151
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                211,
                                                137,
                                                304,
                                                151
                                            ],
                                            "type": "text",
                                            "content": "Q: How many stars can you see on the white board?"
                                        }
                                    ]
                                }
                            ],
                            "index": 3,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                211,
                                152,
                                249,
                                160
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        211,
                                        152,
                                        249,
                                        160
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                211,
                                                152,
                                                249,
                                                160
                                            ],
                                            "type": "text",
                                            "content": "LLaVA-1.5: 1"
                                        }
                                    ]
                                }
                            ],
                            "index": 9,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "image",
                    "bbox": [
                        317,
                        72,
                        399,
                        134
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                317,
                                72,
                                399,
                                134
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        317,
                                        72,
                                        399,
                                        134
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                317,
                                                72,
                                                399,
                                                134
                                            ],
                                            "type": "image",
                                            "image_path": "7375a4a02b4bff12f783af29e785d2dc11520406a11369f713e2e8ab8f2a4e87.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                317,
                                137,
                                405,
                                151
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        317,
                                        137,
                                        405,
                                        151
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                317,
                                                137,
                                                405,
                                                151
                                            ],
                                            "type": "text",
                                            "content": "Q: What candy bar is down there on the bottom?"
                                        }
                                    ]
                                }
                            ],
                            "index": 5,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                317,
                                152,
                                384,
                                161
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        317,
                                        152,
                                        384,
                                        161
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                317,
                                                152,
                                                384,
                                                161
                                            ],
                                            "type": "text",
                                            "content": "LLaVA-1.5: Honey maid"
                                        }
                                    ]
                                }
                            ],
                            "index": 10,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                317,
                                163,
                                378,
                                170
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        317,
                                        163,
                                        378,
                                        170
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                317,
                                                163,
                                                378,
                                                170
                                            ],
                                            "type": "text",
                                            "content": "DeepStack: Hershey's"
                                        }
                                    ]
                                }
                            ],
                            "index": 14,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 4
                },
                {
                    "type": "image",
                    "bbox": [
                        417,
                        72,
                        498,
                        134
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                417,
                                72,
                                498,
                                134
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        417,
                                        72,
                                        498,
                                        134
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                417,
                                                72,
                                                498,
                                                134
                                            ],
                                            "type": "image",
                                            "image_path": "c1c12171c2ffa76c5b1c0328f2b1b589a47b1ed1eb14f944fd6484afa2c390f8.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 6,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                419,
                                137,
                                493,
                                151
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        419,
                                        137,
                                        493,
                                        151
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                419,
                                                137,
                                                493,
                                                151
                                            ],
                                            "type": "text",
                                            "content": "Q: What brand is the mobile phone?"
                                        }
                                    ]
                                }
                            ],
                            "index": 7,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                419,
                                152,
                                478,
                                161
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        419,
                                        152,
                                        478,
                                        161
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                419,
                                                152,
                                                478,
                                                161
                                            ],
                                            "type": "text",
                                            "content": "LLaVA-1.5: Samsung"
                                        }
                                    ]
                                }
                            ],
                            "index": 11,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                419,
                                163,
                                466,
                                170
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        419,
                                        163,
                                        466,
                                        170
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                419,
                                                163,
                                                466,
                                                170
                                            ],
                                            "type": "text",
                                            "content": "DeepStack: HTC"
                                        }
                                    ]
                                }
                            ],
                            "index": 15,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "image",
                    "bbox": [
                        108,
                        189,
                        248,
                        286
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                108,
                                189,
                                248,
                                286
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        108,
                                        189,
                                        248,
                                        286
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                108,
                                                189,
                                                248,
                                                286
                                            ],
                                            "type": "image",
                                            "image_path": "ac0345aef838d744314723592fc7b36add211dc807763913904cc349c8def3d3.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 16,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                110,
                                152,
                                187,
                                161
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        110,
                                        152,
                                        187,
                                        161
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                110,
                                                152,
                                                187,
                                                161
                                            ],
                                            "type": "text",
                                            "content": "LLaVA-1.5: 'Congratulations'"
                                        }
                                    ]
                                }
                            ],
                            "index": 8,
                            "angle": 0,
                            "type": "image_caption"
                        },
                        {
                            "bbox": [
                                211,
                                163,
                                249,
                                170
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        211,
                                        163,
                                        249,
                                        170
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                211,
                                                163,
                                                249,
                                                170
                                            ],
                                            "type": "text",
                                            "content": "DeepStack: 3"
                                        }
                                    ]
                                }
                            ],
                            "index": 13,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 16
                },
                {
                    "bbox": [
                        254,
                        175,
                        489,
                        195
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                254,
                                175,
                                489,
                                195
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        254,
                                        175,
                                        489,
                                        195
                                    ],
                                    "type": "text",
                                    "content": "Q: Please describe in as much detail as possible what each person in the picture is doing.  \nLLaVA-1.5:"
                                }
                            ]
                        }
                    ],
                    "index": 17
                },
                {
                    "bbox": [
                        255,
                        195,
                        498,
                        251
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                195,
                                498,
                                251
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        195,
                                        498,
                                        251
                                    ],
                                    "type": "text",
                                    "content": "In the image, there is a group of people gathered around a cow in a grassy field. A woman is standing next to the cow, possibly petting it or tending to it. Another woman is standing nearby, possibly observing the scene or waiting for her turn to interact with the cow. There are several other people in the background, some of whom are standing closer to the cow, while others are further away. A few individuals are wearing ties, which suggests a more formal or professional setting. In the scene, there are also a couple of chairs and a dining table, indicating that this gathering might be a social event or a meeting. A handbag can be seen placed on the ground, possibly belonging to one of the attendees."
                                }
                            ]
                        }
                    ],
                    "index": 18
                },
                {
                    "bbox": [
                        255,
                        254,
                        288,
                        262
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                254,
                                288,
                                262
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        254,
                                        288,
                                        262
                                    ],
                                    "type": "text",
                                    "content": "DeepStack:"
                                }
                            ]
                        }
                    ],
                    "index": 19
                },
                {
                    "bbox": [
                        255,
                        263,
                        499,
                        306
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                263,
                                499,
                                306
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        263,
                                        499,
                                        306
                                    ],
                                    "type": "text",
                                    "content": "In the image, there is a group of people gathered around a brown and white cow. A woman is standing next to the cow, holding its halter, while a man is standing nearby, possibly observing the scene. Other people are also present in the area, some standing closer to the cow and others further away. There are a few chairs scattered around the scene, with one near the center and two others closer to the right side. A truck can be seen in the background possibly indicating that the event is taking place at a fair or similar outdoor venue."
                                }
                            ]
                        }
                    ],
                    "index": 20
                },
                {
                    "type": "image",
                    "bbox": [
                        108,
                        312,
                        252,
                        439
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                108,
                                312,
                                252,
                                439
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        108,
                                        312,
                                        252,
                                        439
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                108,
                                                312,
                                                252,
                                                439
                                            ],
                                            "type": "image",
                                            "image_path": "166e09de79064747594bfc3ba37119432ac2de576264a9360fb29d91de6240a8.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 21,
                            "angle": 0,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                104,
                                450,
                                504,
                                491
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        450,
                                        504,
                                        491
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                450,
                                                504,
                                                491
                                            ],
                                            "type": "text",
                                            "content": "Figure 4: Visualization. Both LLaVA-1.5 and DeepStack use 576 visual context length for a fair comparison. Top: We mark the area corresponding to each question with a red circle. DeepStack can well answer the questions which need high-resolution and fine-grained understanding. Bottom: DeepStack demonstrates a more accurate visual understanding in detailed visual captioning."
                                        }
                                    ]
                                }
                            ],
                            "index": 27,
                            "angle": 0,
                            "type": "image_caption"
                        }
                    ],
                    "index": 21
                },
                {
                    "bbox": [
                        254,
                        316,
                        394,
                        323
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                254,
                                316,
                                394,
                                323
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        254,
                                        316,
                                        394,
                                        323
                                    ],
                                    "type": "text",
                                    "content": "Q: Please describe the image as detailed as possible."
                                }
                            ]
                        }
                    ],
                    "index": 22
                },
                {
                    "bbox": [
                        255,
                        326,
                        286,
                        333
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                326,
                                286,
                                333
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        326,
                                        286,
                                        333
                                    ],
                                    "type": "text",
                                    "content": "LLaVA-1.5:"
                                }
                            ]
                        }
                    ],
                    "index": 23
                },
                {
                    "bbox": [
                        255,
                        334,
                        500,
                        376
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                334,
                                500,
                                376
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        334,
                                        500,
                                        376
                                    ],
                                    "type": "text",
                                    "content": "The image displays a graph with a central circle that represents a VOQA (Voice over QAM) system. The circle is divided into four quadrants, each representing a different aspect of the system. The quadrants are labeled with the acronyms VOQA, POP, GAOA, and TEX. The graph also includes several smaller circles scattered around the main circle, which are likely related to the VOQA system as well. These smaller circles are positioned at various distances from the main circle, indicating the different levels of the system."
                                }
                            ]
                        }
                    ],
                    "index": 24
                },
                {
                    "bbox": [
                        255,
                        380,
                        288,
                        388
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                380,
                                288,
                                388
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        380,
                                        288,
                                        388
                                    ],
                                    "type": "text",
                                    "content": "DeepStack:"
                                }
                            ]
                        }
                    ],
                    "index": 25
                },
                {
                    "bbox": [
                        255,
                        390,
                        498,
                        439
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                255,
                                390,
                                498,
                                439
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        255,
                                        390,
                                        498,
                                        439
                                    ],
                                    "type": "text",
                                    "content": "The image features a circular diagram with various labels and numbers. The diagram is divided into different sections, each with its own set of labels and numbers. The central section of the diagram is labeled \"VQA2,\" while the surrounding sections are labeled \"Pope,\" \"GQA,\" \"Seed,\" \"InfoVQA,\" \"DocVQA,\" and \"Sequence.\" There are several numbers in the diagram, with some located in the central section and others scattered throughout the different sections. The numbers are likely used to represent data or information related to the various labels and sections of the diagram."
                                }
                            ]
                        }
                    ],
                    "index": 26
                },
                {
                    "bbox": [
                        104,
                        513,
                        504,
                        568
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                513,
                                504,
                                568
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        513,
                                        504,
                                        568
                                    ],
                                    "type": "text",
                                    "content": "DeepStack boosts LMMs from high-resolution tokens, not residual connections. We experiment to assess the impact of high-resolution images and residual connections in DeepStack by stacking original visual tokens into different layers. As shown in Tab. 6, stacking repeated original tokens (dummy tokens) does not improve performance. This indicates that the performance boost in DeepStack comes from the high-resolution tokens, not from the residual connections."
                                }
                            ]
                        }
                    ],
                    "index": 28
                },
                {
                    "type": "table",
                    "bbox": [
                        123,
                        578,
                        486,
                        628
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                123,
                                578,
                                486,
                                628
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        123,
                                        578,
                                        486,
                                        628
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                123,
                                                578,
                                                486,
                                                628
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Tok. Enhance</td><td>Stack Tok.</td><td>GQA</td><td>POPE</td><td>SEED</td><td>TextVQA</td><td>DocVQA</td><td>ChartQA</td><td>InfoVQA</td><td>AVG</td></tr><tr><td>None</td><td>None</td><td>62.5</td><td>85.5</td><td>63.5</td><td>56.7</td><td>31.7</td><td>15.8</td><td>28.3</td><td>49.1</td></tr><tr><td>DeepStack</td><td>Dummy</td><td>62.2</td><td>85.3</td><td>63.8</td><td>56.9</td><td>31.2</td><td>15.4</td><td>28.8</td><td>49.1</td></tr><tr><td>DeepStack</td><td>Hi-Res</td><td>63.0</td><td>86.4</td><td>62.9</td><td>58.8</td><td>38.7</td><td>17.2</td><td>30.8</td><td>51.1</td></tr></table>",
                                            "image_path": "03b011c94db68f05d83ae5fcc18951f0e92bfcf96b3ea607c924fbaf94333905.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 29,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                628,
                                504,
                                649
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        628,
                                        504,
                                        649
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                628,
                                                504,
                                                649
                                            ],
                                            "type": "text",
                                            "content": "Table 6: Ablations on high-resolution visual tokens for stacking. Dummy refers to repeating the original visual tokens for token stacking; Hi-Res is our default setting that uses high-resolution visual tokens for stacking."
                                        }
                                    ]
                                }
                            ],
                            "index": 30,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 29
                },
                {
                    "bbox": [
                        104,
                        677,
                        506,
                        723
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                677,
                                506,
                                723
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        677,
                                        506,
                                        723
                                    ],
                                    "type": "text",
                                    "content": "DeepStack achieves a better trade-off between performance and effectiveness. We compare DeepStack with other token enhancement strategies, including dimension-wise concatenation, sequence-wise with high-resolution visual tokens, and string both global visual and high-resolution tokens. As shown in Tab. 7, although string-based methods can bring significant improvement on some"
                                }
                            ]
                        }
                    ],
                    "index": 31
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "10"
                                }
                            ]
                        }
                    ],
                    "index": 32
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 9
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        104,
                        72,
                        504,
                        106
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                72,
                                504,
                                106
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        72,
                                        504,
                                        106
                                    ],
                                    "type": "text",
                                    "content": "benchmarks, they increase the number of tokens at the same time, which will increase the training and inference cost. Meanwhile, DeepStack achieves the best trade-off between performance and effectiveness without introducing extra visual tokens."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "type": "table",
                    "bbox": [
                        106,
                        115,
                        504,
                        183
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                115,
                                504,
                                183
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        115,
                                        504,
                                        183
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                115,
                                                504,
                                                183
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Tok. Enhance</td><td>N Tok.</td><td>Eff. Tok.</td><td>GQA</td><td>POPE</td><td>SEED</td><td>TextVQA</td><td>DocVQA</td><td>ChartQA</td><td>InfoVQA</td><td>AVG</td></tr><tr><td>None</td><td>576</td><td>576</td><td>62.5</td><td>85.5</td><td>63.5</td><td>56.7</td><td>31.7</td><td>15.8</td><td>28.3</td><td>49.1</td></tr><tr><td>Dimension Concat</td><td>576</td><td>2880</td><td>59.5</td><td>86.3</td><td>62.9</td><td>56.4</td><td>35.9</td><td>16.4</td><td>28.5</td><td>49.4</td></tr><tr><td>Hi-Res String</td><td>2304</td><td>2304</td><td>61.8</td><td>86.2</td><td>62.1</td><td>55.0</td><td>43.5</td><td>16.2</td><td>30.4</td><td>50.7</td></tr><tr><td>Global+ Hi-Res String</td><td>2880</td><td>2880</td><td>62.3</td><td>86.4</td><td>62.6</td><td>54.7</td><td>43.3</td><td>16.7</td><td>31.2</td><td>51.0</td></tr><tr><td>DeepStack</td><td>576</td><td>2880</td><td>63.0</td><td>86.4</td><td>62.9</td><td>58.8</td><td>38.7</td><td>17.2</td><td>30.8</td><td>51.1</td></tr></table>",
                                            "image_path": "4b5fe9002861004d0a5e403476ea1c83ed1432739d4b875f5e94be2852c3d440.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 1,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                184,
                                504,
                                214
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        184,
                                        504,
                                        214
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "text",
                                            "content": "Table 7: Ablations on different token enhancement strategies. Dimension Concat refers to concatenate X and "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\mathbf{X}^{\\text{stack}}"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "text",
                                            "content": " via the channel of features hidden space; Hi-Res String and Global+Hi-Res String refers to string "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "inline_equation",
                                            "content": "\\mathbf{X}^{\\text{stack}}"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "text",
                                            "content": " and "
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "inline_equation",
                                            "content": "[\\mathbf{X},\\mathbf{X}^{\\text{stack}}]"
                                        },
                                        {
                                            "bbox": [
                                                104,
                                                184,
                                                504,
                                                214
                                            ],
                                            "type": "text",
                                            "content": " via sequence, respectively."
                                        }
                                    ]
                                }
                            ],
                            "index": 2,
                            "angle": 0,
                            "type": "table_footnote"
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        236,
                        506,
                        304
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                236,
                                506,
                                304
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        236,
                                        506,
                                        304
                                    ],
                                    "type": "text",
                                    "content": "DeepStack unleashes the power after fine-tuning the image encoder. We further experiment with how DeepStack compared coporated with fine-tuning backbones. As shown in Tab. 4, DeepStack achieves the best performance when fine-tuning the backbone. It is worth noticing that when fine-tuning the backbone without DeepStack, the improvement is limited. After combining backbone fietuning with DeepStack, the performance significantly increases among different benchmarks. It is because of the deep interaction between visual tokens and the LLM decoder."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "table",
                    "bbox": [
                        127,
                        312,
                        483,
                        373
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                127,
                                312,
                                483,
                                373
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        127,
                                        312,
                                        483,
                                        373
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                127,
                                                312,
                                                483,
                                                373
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Tok. Enhance</td><td>Ft Enc.</td><td>GQA</td><td>POPE</td><td>SEED</td><td>TextVQA</td><td>DocVQA</td><td>ChartQA</td><td>InfoVQA</td><td>AVG</td></tr><tr><td>None</td><td></td><td>62.5</td><td>85.5</td><td>63.5</td><td>56.7</td><td>31.7</td><td>15.8</td><td>28.3</td><td>49.1</td></tr><tr><td>None</td><td>✓</td><td>62.4</td><td>85.8</td><td>64.0</td><td>56.1</td><td>27.5</td><td>15.3</td><td>28.3</td><td>48.5</td></tr><tr><td>DeepStack</td><td></td><td>63.0</td><td>86.4</td><td>62.9</td><td>58.8</td><td>38.7</td><td>17.2</td><td>30.8</td><td>51.1</td></tr><tr><td>DeepStack</td><td>✓</td><td>63.1</td><td>86.8</td><td>63.9</td><td>61.1</td><td>41.2</td><td>18.9</td><td>31.5</td><td>52.4</td></tr></table>",
                                            "image_path": "34455ed04cb4507f92d3df8f550bd78bf72ae4784044518d9ebd9cf68696ad69.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                104,
                                373,
                                504,
                                393
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        104,
                                        373,
                                        504,
                                        393
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                104,
                                                373,
                                                504,
                                                393
                                            ],
                                            "type": "text",
                                            "content": "Table 8: Ablations on fine-tuning vision encoder. DeepStack achieves best performance after fine-tuning vision encoder."
                                        }
                                    ]
                                }
                            ],
                            "index": 5,
                            "angle": 0,
                            "type": "table_footnote"
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        105,
                        423,
                        185,
                        436
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                423,
                                185,
                                436
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        423,
                                        185,
                                        436
                                    ],
                                    "type": "text",
                                    "content": "5 Conclusion"
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        104,
                        449,
                        506,
                        559
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                449,
                                506,
                                559
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        449,
                                        506,
                                        559
                                    ],
                                    "type": "text",
                                    "content": "In this work, we had presented DeepStack, a simple yet effective way to connect vision and language in the context of LMMs. Unlike previous works that always string (compressed) visual tokens into a sequence, we alternatively introduced a new perspective on transformer decoder layers in LLMs, and proposed a DeepStack strategy to feed different visual tokens into different layers of LLMs. This strategy significantly mitigates the efficiency overhead introduced by visual tokens and makes it possible to convey more visual information to LLMs. As a result, our DeepStack demonstrated consistent improvements over two baseline models across a wide range of benchmarks. The benefits are particularly significant on tasks that inherently require more tokens, such as high-resolution image understanding. We hope this new DeepStack strategy could open up new ideas on how to connect vision and language for faster and better multimodal models in the regime of LMMs."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        104,
                        563,
                        504,
                        629
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                563,
                                504,
                                629
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        563,
                                        504,
                                        629
                                    ],
                                    "type": "text",
                                    "content": "Limitation and Future Works. Our current DeepStack simply inserts the visual tokens into middle LLMs layers via a residual connection in a heuristic manner. Though it already exhibits promising results, we may find a more powerful way to infuse the visual information, e.g., through gated function or layer-wise positional embeddings. Meanwhile, how to systematically decide the starting layer and number of layers also deserves more study. We leave these as promising directions to explore in the future."
                                }
                            ]
                        }
                    ],
                    "index": 8
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        310,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                310,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        310,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "11"
                                }
                            ]
                        }
                    ],
                    "index": 9
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 10
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        71,
                        165,
                        84
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                106,
                                71,
                                165,
                                84
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        106,
                                        71,
                                        165,
                                        84
                                    ],
                                    "type": "text",
                                    "content": "References"
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        106,
                        89,
                        505,
                        722
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 21,
                    "blocks": [
                        {
                            "bbox": [
                                111,
                                89,
                                505,
                                120
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        89,
                                        505,
                                        120
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                89,
                                                505,
                                                120
                                            ],
                                            "type": "text",
                                            "content": "[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                111,
                                127,
                                505,
                                148
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        127,
                                        505,
                                        148
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                127,
                                                505,
                                                148
                                            ],
                                            "type": "text",
                                            "content": "[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                111,
                                156,
                                505,
                                176
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        156,
                                        505,
                                        176
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                156,
                                                505,
                                                176
                                            ],
                                            "type": "text",
                                            "content": "[3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                111,
                                182,
                                505,
                                244
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        182,
                                        505,
                                        244
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                182,
                                                505,
                                                244
                                            ],
                                            "type": "text",
                                            "content": "[4] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: a visual language model for few-shot learning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Adv. Neural Inform. Process. Syst., volume 35, pages 23716-23736. Curran Associates, Inc., 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                111,
                                251,
                                505,
                                272
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        251,
                                        505,
                                        272
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                251,
                                                505,
                                                272
                                            ],
                                            "type": "text",
                                            "content": "[5] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                111,
                                279,
                                505,
                                300
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        279,
                                        505,
                                        300
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                279,
                                                505,
                                                300
                                            ],
                                            "type": "text",
                                            "content": "[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. NeurIPS, 2020."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                111,
                                307,
                                505,
                                327
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        307,
                                        505,
                                        327
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                307,
                                                505,
                                                327
                                            ],
                                            "type": "text",
                                            "content": "[7] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                111,
                                335,
                                505,
                                346
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        335,
                                        505,
                                        346
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                335,
                                                505,
                                                346
                                            ],
                                            "type": "text",
                                            "content": "[8] J. Cha, W. Kang, J. Mun, and B. Roh. Honeybee: Locality-enhanced projector for multimodal llm, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                111,
                                354,
                                505,
                                374
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        111,
                                        354,
                                        505,
                                        374
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                111,
                                                354,
                                                505,
                                                374
                                            ],
                                            "type": "text",
                                            "content": "[9] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                106,
                                381,
                                483,
                                392
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        381,
                                        483,
                                        392
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                381,
                                                483,
                                                392
                                            ],
                                            "type": "text",
                                            "content": "[10] D. Chen and W. B. Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                106,
                                399,
                                505,
                                430
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        399,
                                        505,
                                        430
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                399,
                                                505,
                                                430
                                            ],
                                            "type": "text",
                                            "content": "[11] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and M. Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                106,
                                437,
                                505,
                                458
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        437,
                                        505,
                                        458
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                437,
                                                505,
                                                458
                                            ],
                                            "type": "text",
                                            "content": "[12] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                106,
                                465,
                                505,
                                486
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        465,
                                        505,
                                        486
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                465,
                                                505,
                                                486
                                            ],
                                            "type": "text",
                                            "content": "[13] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                106,
                                493,
                                505,
                                514
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        493,
                                        505,
                                        514
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                493,
                                                505,
                                                514
                                            ],
                                            "type": "text",
                                            "content": "[14] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. TPAMI, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                106,
                                521,
                                505,
                                551
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        521,
                                        505,
                                        551
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                521,
                                                505,
                                                551
                                            ],
                                            "type": "text",
                                            "content": "[15] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with "
                                        },
                                        {
                                            "bbox": [
                                                106,
                                                521,
                                                505,
                                                551
                                            ],
                                            "type": "inline_equation",
                                            "content": "90\\%"
                                        },
                                        {
                                            "bbox": [
                                                106,
                                                521,
                                                505,
                                                551
                                            ],
                                            "type": "text",
                                            "content": " * chatgpt quality, March 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                106,
                                559,
                                505,
                                580
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        559,
                                        505,
                                        580
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                559,
                                                505,
                                                580
                                            ],
                                            "type": "text",
                                            "content": "[16] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. NeurIPS, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                106,
                                587,
                                505,
                                609
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        587,
                                        505,
                                        609
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                587,
                                                505,
                                                609
                                            ],
                                            "type": "text",
                                            "content": "[17] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. A. Li, P. Fung, and S. C. H. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. ArXiv, abs/2305.06500, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                106,
                                615,
                                505,
                                635
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        615,
                                        505,
                                        635
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                615,
                                                505,
                                                635
                                            ],
                                            "type": "text",
                                            "content": "[18] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                106,
                                643,
                                505,
                                673
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        643,
                                        505,
                                        673
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                643,
                                                505,
                                                673
                                            ],
                                            "type": "text",
                                            "content": "[19] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, S. Zhang, H. Duan, W. Zhang, Y. Li, et al. Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd. arXiv preprint arXiv:2404.06512, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                106,
                                681,
                                505,
                                722
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        681,
                                        505,
                                        722
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                681,
                                                505,
                                                722
                                            ],
                                            "type": "text",
                                            "content": "[20] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, S. Zhang, H. Duan, W. Zhang, Y. Li, H. Yan, Y. Gao, Z. Chen, X. Zhang, W. Li, J. Li, W. Wang, K. Chen, C. He, X. Zhang, J. Dai, Y. Qiao, D. Lin, and J. Wang. Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "12"
                                }
                            ]
                        }
                    ],
                    "index": 22
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 11
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        72,
                        506,
                        722
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 22,
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                72,
                                505,
                                95
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        72,
                                        505,
                                        95
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                72,
                                                505,
                                                95
                                            ],
                                            "type": "text",
                                            "content": "[21] X. Fan, T. Ji, C. Jiang, S. Li, S. Jin, S. Song, J. Wang, B. Hong, L. Chen, G. Zheng, et al. Mousi: Poly-visual-expert vision-language models. arXiv preprint arXiv:2401.17221, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                106,
                                100,
                                506,
                                131
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        100,
                                        506,
                                        131
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                100,
                                                506,
                                                131
                                            ],
                                            "type": "text",
                                            "content": "[22] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, et al. *Sphinx-x: Scaling data and parameters for a family of multi-modal large language models.* arXiv preprint arXiv:2402.05935, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                107,
                                139,
                                504,
                                160
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        139,
                                        504,
                                        160
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                139,
                                                504,
                                                160
                                            ],
                                            "type": "text",
                                            "content": "[23] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                107,
                                167,
                                505,
                                188
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        167,
                                        505,
                                        188
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                167,
                                                505,
                                                188
                                            ],
                                            "type": "text",
                                            "content": "[24] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                107,
                                194,
                                504,
                                216
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        194,
                                        504,
                                        216
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                194,
                                                504,
                                                216
                                            ],
                                            "type": "text",
                                            "content": "[25] D. A. Hudson and C. D. Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                107,
                                223,
                                504,
                                243
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        223,
                                        504,
                                        243
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                223,
                                                504,
                                                243
                                            ],
                                            "type": "text",
                                            "content": "[26] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General perception with iterative attention. In ICML, 2021."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                107,
                                251,
                                506,
                                281
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        251,
                                        506,
                                        281
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                251,
                                                506,
                                                281
                                            ],
                                            "type": "text",
                                            "content": "[27] M. Javaheripi, S. Bubeck, M. Abdin, J. Aneja, S. Bubeck, C. C. T. Mendes, W. Chen, A. Del Giorno, R. Eldan, S. Gopi, et al. Phi-2: The surprising power of small language models. *Microsoft Research Blog*, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                107,
                                289,
                                504,
                                310
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        289,
                                        504,
                                        310
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                289,
                                                504,
                                                310
                                            ],
                                            "type": "text",
                                            "content": "[28] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                107,
                                316,
                                504,
                                338
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        316,
                                        504,
                                        338
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                316,
                                                504,
                                                338
                                            ],
                                            "type": "text",
                                            "content": "[29] K. Kafle, B. Price, S. Cohen, and C. Kanan. Dvqa: Understanding data visualizations via question answering. In CVPR, 2018."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                107,
                                345,
                                504,
                                365
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        345,
                                        504,
                                        365
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                345,
                                                504,
                                                365
                                            ],
                                            "type": "text",
                                            "content": "[30] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                107,
                                373,
                                504,
                                394
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        373,
                                        504,
                                        394
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                373,
                                                504,
                                                394
                                            ],
                                            "type": "text",
                                            "content": "[31] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. A diagram is worth a dozen images. In ECCV, 2016."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                107,
                                400,
                                504,
                                422
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        400,
                                        504,
                                        422
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                400,
                                                504,
                                                422
                                            ],
                                            "type": "text",
                                            "content": "[32] G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. Ocr-free document understanding transformer. In ECCV, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                107,
                                429,
                                504,
                                449
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        429,
                                        504,
                                        449
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                429,
                                                504,
                                                449
                                            ],
                                            "type": "text",
                                            "content": "[33] W. Kim, C. Choi, W. Lee, and W. Rhee. An image grid can be worth a video: Zero-shot video question answering using a vlm. arXiv preprint arXiv:2403.18406, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                107,
                                456,
                                506,
                                487
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        456,
                                        506,
                                        487
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                456,
                                                506,
                                                487
                                            ],
                                            "type": "text",
                                            "content": "[34] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                107,
                                495,
                                380,
                                506
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        495,
                                        380,
                                        506
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                495,
                                                380,
                                                506
                                            ],
                                            "type": "text",
                                            "content": "[35] LAION-4V. https://huggingface.co/datasets/laion/gpt4v-dataset, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                107,
                                513,
                                504,
                                534
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        513,
                                        504,
                                        534
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                513,
                                                504,
                                                534
                                            ],
                                            "type": "text",
                                            "content": "[36] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                107,
                                540,
                                506,
                                571
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        540,
                                        506,
                                        571
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                540,
                                                506,
                                                571
                                            ],
                                            "type": "text",
                                            "content": "[37] H. Laurencon, D. van Strien, S. Bekman, L. Tronchon, L. Saulnier, T. Wang, S. Karamcheti, A. Singh, G. Pistilli, Y. Jernite, et al. Introducing idefics: An open reproduction of state-of-the-art visual language model, 2023. URL https://huggingface.co/blog/idefics.Accessed, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                107,
                                578,
                                506,
                                600
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        578,
                                        506,
                                        600
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                578,
                                                506,
                                                600
                                            ],
                                            "type": "text",
                                            "content": "[38] T. Le Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                107,
                                606,
                                506,
                                638
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        606,
                                        506,
                                        638
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                606,
                                                506,
                                                638
                                            ],
                                            "type": "text",
                                            "content": "[39] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                107,
                                644,
                                504,
                                666
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        644,
                                        504,
                                        666
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                644,
                                                504,
                                                666
                                            ],
                                            "type": "text",
                                            "content": "[40] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                107,
                                673,
                                504,
                                694
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        673,
                                        504,
                                        694
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                673,
                                                504,
                                                694
                                            ],
                                            "type": "text",
                                            "content": "[41] B. Li, P. Zhang, K. Zhang, F. Pu, X. Du, Y. Dong, H. Liu, Y. Zhang, G. Zhang, C. Li, and Z. Liu. Lmms-eval: Accelerating the development of large multimodal models, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        },
                        {
                            "bbox": [
                                107,
                                700,
                                504,
                                722
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        700,
                                        504,
                                        722
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                700,
                                                504,
                                                722
                                            ],
                                            "type": "text",
                                            "content": "[42] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 21
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        310,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                310,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        310,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "13"
                                }
                            ]
                        }
                    ],
                    "index": 23
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 12
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        72,
                        506,
                        720
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 22,
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                72,
                                505,
                                95
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        72,
                                        505,
                                        95
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                72,
                                                505,
                                                95
                                            ],
                                            "type": "text",
                                            "content": "[43] J. Li, D. Li, S. Savarese, and S. C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                106,
                                101,
                                506,
                                122
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        101,
                                        506,
                                        122
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                101,
                                                506,
                                                122
                                            ],
                                            "type": "text",
                                            "content": "[44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                107,
                                129,
                                504,
                                152
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        129,
                                        504,
                                        152
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                129,
                                                504,
                                                152
                                            ],
                                            "type": "text",
                                            "content": "[45] J. Li, D. Li, C. Xiong, and S. C. H. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                106,
                                158,
                                504,
                                179
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        158,
                                        504,
                                        179
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                158,
                                                504,
                                                179
                                            ],
                                            "type": "text",
                                            "content": "[46] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                107,
                                186,
                                504,
                                208
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        186,
                                        504,
                                        208
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                186,
                                                504,
                                                208
                                            ],
                                            "type": "text",
                                            "content": "[47] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                107,
                                215,
                                504,
                                246
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        215,
                                        504,
                                        246
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                215,
                                                504,
                                                246
                                            ],
                                            "type": "text",
                                            "content": "[48] Z. Lin, C. Liu, R. Zhang, P. Gao, L. Qiu, H. Xiao, H. Qiu, C. Lin, W. Shao, K. Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                107,
                                254,
                                505,
                                274
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        254,
                                        505,
                                        274
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                254,
                                                505,
                                                274
                                            ],
                                            "type": "text",
                                            "content": "[49] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. ArXiv, abs/2310.03744, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                107,
                                282,
                                504,
                                303
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        282,
                                        504,
                                        303
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                282,
                                                504,
                                                303
                                            ],
                                            "type": "text",
                                            "content": "[50] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-last: Improved reasoning,OCR, and world knowledge, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                107,
                                311,
                                418,
                                322
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        311,
                                        418,
                                        322
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                311,
                                                418,
                                                322
                                            ],
                                            "type": "text",
                                            "content": "[51] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                107,
                                329,
                                504,
                                350
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        329,
                                        504,
                                        350
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                329,
                                                504,
                                                350
                                            ],
                                            "type": "text",
                                            "content": "[52] K. Mangalam, R. Akshulakov, and J. Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. NeurIPS, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                107,
                                357,
                                504,
                                379
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        357,
                                        504,
                                        379
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                357,
                                                504,
                                                379
                                            ],
                                            "type": "text",
                                            "content": "[53] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In CVPR, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                107,
                                386,
                                504,
                                407
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        386,
                                        504,
                                        407
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                386,
                                                504,
                                                407
                                            ],
                                            "type": "text",
                                            "content": "[54] A. Masy, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                107,
                                415,
                                506,
                                426
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        415,
                                        506,
                                        426
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                415,
                                                506,
                                                426
                                            ],
                                            "type": "text",
                                            "content": "[55] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. Infographicvqa. In WACV, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                107,
                                434,
                                506,
                                453
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        434,
                                        506,
                                        453
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                434,
                                                506,
                                                453
                                            ],
                                            "type": "text",
                                            "content": "[56] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: A dataset for vqa on document images. In WACV, 2021."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                107,
                                461,
                                506,
                                503
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        461,
                                        506,
                                        503
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                461,
                                                506,
                                                503
                                            ],
                                            "type": "text",
                                            "content": "[57] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers, A. Belyi, H. Zhang, K. Singh, D. Kang, A. Jain, H. He, M. Schwarzer, T. Gunter, X. Kong, A. Zhang, J. Wang, C. Wang, N. Du, T. Lei, S. Wiseman, G. Yin, M. Lee, Z. Wang, R. Pang, P. Grasch, A. Toshev, and Y. Yang. Mm1: Methods, analysis & insights from multimodal llm pre-training, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                107,
                                510,
                                504,
                                531
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        510,
                                        504,
                                        531
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                510,
                                                504,
                                                531
                                            ],
                                            "type": "text",
                                            "content": "[58] A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                107,
                                539,
                                506,
                                559
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        539,
                                        506,
                                        559
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                539,
                                                506,
                                                559
                                            ],
                                            "type": "text",
                                            "content": "[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                107,
                                567,
                                506,
                                587
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        567,
                                        506,
                                        587
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                567,
                                                506,
                                                587
                                            ],
                                            "type": "text",
                                            "content": "[60] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                107,
                                596,
                                506,
                                626
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        596,
                                        506,
                                        626
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                596,
                                                506,
                                                626
                                            ],
                                            "type": "text",
                                            "content": "[61] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                107,
                                634,
                                504,
                                655
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        634,
                                        504,
                                        655
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                634,
                                                504,
                                                655
                                            ],
                                            "type": "text",
                                            "content": "[62] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding by generative pre-training. 2018."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                107,
                                662,
                                504,
                                683
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        662,
                                        504,
                                        683
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                662,
                                                504,
                                                683
                                            ],
                                            "type": "text",
                                            "content": "[63] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        },
                        {
                            "bbox": [
                                107,
                                691,
                                506,
                                720
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        691,
                                        506,
                                        720
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                691,
                                                506,
                                                720
                                            ],
                                            "type": "text",
                                            "content": "[64] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 2020."
                                        }
                                    ]
                                }
                            ],
                            "index": 21
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "14"
                                }
                            ]
                        }
                    ],
                    "index": 23
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 13
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        72,
                        506,
                        722
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 21,
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                72,
                                505,
                                103
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        72,
                                        505,
                                        103
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                72,
                                                505,
                                                103
                                            ],
                                            "type": "text",
                                            "content": "[65] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili'c, D. Hesslow, R. Castagn'e, A. S. Luccioni, F. Yvon, and M. G. et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                106,
                                110,
                                506,
                                152
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        110,
                                        506,
                                        152
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                110,
                                                506,
                                                152
                                            ],
                                            "type": "text",
                                            "content": "[66] C. Schuhmann, R. Beaumont, R. Vencu, C. W. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. R. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                107,
                                159,
                                504,
                                181
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        159,
                                        504,
                                        181
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                159,
                                                504,
                                                181
                                            ],
                                            "type": "text",
                                            "content": "[67] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. In ECCV, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                107,
                                188,
                                295,
                                199
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        188,
                                        295,
                                        199
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                188,
                                                295,
                                                199
                                            ],
                                            "type": "text",
                                            "content": "[68] ShareGPT. https://sharegpt.com/, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        },
                        {
                            "bbox": [
                                107,
                                206,
                                504,
                                227
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        206,
                                        504,
                                        227
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                206,
                                                504,
                                                227
                                            ],
                                            "type": "text",
                                            "content": "[69] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa models that can read. In CVPR, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 4
                        },
                        {
                            "bbox": [
                                107,
                                235,
                                505,
                                255
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        235,
                                        505,
                                        255
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                235,
                                                505,
                                                255
                                            ],
                                            "type": "text",
                                            "content": "[70] Q. Sun, Y. Cui, X. Zhang, F. Zhang, Q. Yu, Z. Luo, Y. Wang, Y. Rao, J. Liu, T. Huang, and X. Wang. Generative multimodal models are in-context learners, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 5
                        },
                        {
                            "bbox": [
                                107,
                                263,
                                506,
                                295
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        263,
                                        506,
                                        295
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                263,
                                                506,
                                                295
                                            ],
                                            "type": "text",
                                            "content": "[71] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm.stanford.edu/2023/03/13/alpaca.html, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 6
                        },
                        {
                            "bbox": [
                                107,
                                301,
                                504,
                                323
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        301,
                                        504,
                                        323
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                301,
                                                504,
                                                323
                                            ],
                                            "type": "text",
                                            "content": "[72] R. Tito, D. Karatzas, and E. Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 7
                        },
                        {
                            "bbox": [
                                107,
                                330,
                                504,
                                351
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        330,
                                        504,
                                        351
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                330,
                                                504,
                                                351
                                            ],
                                            "type": "text",
                                            "content": "[73] S. Tong, Z. Liu, Y. Zhai, Y. Ma, Y. LeCun, and S. Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. arXiv preprint arXiv:2401.06209, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 8
                        },
                        {
                            "bbox": [
                                107,
                                358,
                                506,
                                388
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        358,
                                        506,
                                        388
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                358,
                                                506,
                                                388
                                            ],
                                            "type": "text",
                                            "content": "[74] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 9
                        },
                        {
                            "bbox": [
                                107,
                                396,
                                506,
                                418
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        396,
                                        506,
                                        418
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                396,
                                                506,
                                                418
                                            ],
                                            "type": "text",
                                            "content": "[75] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. NeurIPS, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 10
                        },
                        {
                            "bbox": [
                                107,
                                425,
                                504,
                                446
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        425,
                                        504,
                                        446
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                425,
                                                504,
                                                446
                                            ],
                                            "type": "text",
                                            "content": "[76] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang. To see is to believe: Prompting gpt-4v for better visual instruction tuning. arXiv preprint arXiv:2311.07574, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 11
                        },
                        {
                            "bbox": [
                                106,
                                453,
                                504,
                                475
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        453,
                                        504,
                                        475
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                453,
                                                504,
                                                475
                                            ],
                                            "type": "text",
                                            "content": "[77] Z. Wu, Z. Weng, W. Peng, X. Yang, A. Li, L. S. Davis, and Y.-G. Jiang. Building an open-vocabulary video clip model with better architectures, optimization and data. TPAMI, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 12
                        },
                        {
                            "bbox": [
                                107,
                                482,
                                504,
                                503
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        482,
                                        504,
                                        503
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                482,
                                                504,
                                                503
                                            ],
                                            "type": "text",
                                            "content": "[78] J. Xiao, X. Shang, A. Yao, and T.-S. Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, 2021."
                                        }
                                    ]
                                }
                            ],
                            "index": 13
                        },
                        {
                            "bbox": [
                                107,
                                510,
                                504,
                                532
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        510,
                                        504,
                                        532
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                510,
                                                504,
                                                532
                                            ],
                                            "type": "text",
                                            "content": "[79] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering via gradually refined attention over appearance and motion. In MM, 2017."
                                        }
                                    ]
                                }
                            ],
                            "index": 14
                        },
                        {
                            "bbox": [
                                107,
                                539,
                                504,
                                559
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        539,
                                        504,
                                        559
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                539,
                                                504,
                                                559
                                            ],
                                            "type": "text",
                                            "content": "[80] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015."
                                        }
                                    ]
                                }
                            ],
                            "index": 15
                        },
                        {
                            "bbox": [
                                107,
                                567,
                                504,
                                589
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        567,
                                        504,
                                        589
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                567,
                                                504,
                                                589
                                            ],
                                            "type": "text",
                                            "content": "[81] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 16
                        },
                        {
                            "bbox": [
                                107,
                                596,
                                504,
                                616
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        596,
                                        504,
                                        616
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                596,
                                                504,
                                                616
                                            ],
                                            "type": "text",
                                            "content": "[82] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In AAAI, 2019."
                                        }
                                    ]
                                }
                            ],
                            "index": 17
                        },
                        {
                            "bbox": [
                                107,
                                624,
                                504,
                                655
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        624,
                                        504,
                                        655
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                624,
                                                504,
                                                655
                                            ],
                                            "type": "text",
                                            "content": "[83] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 18
                        },
                        {
                            "bbox": [
                                107,
                                662,
                                504,
                                684
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        662,
                                        504,
                                        684
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                662,
                                                504,
                                                684
                                            ],
                                            "type": "text",
                                            "content": "[84] K. Yun, J. Honorio, D. Chattopadhyay, T. L. Berg, and D. Samaras. Two-person interaction detection using body-pose features and multiple instance learning. In CVPR, 2012."
                                        }
                                    ]
                                }
                            ],
                            "index": 19
                        },
                        {
                            "bbox": [
                                107,
                                691,
                                504,
                                722
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        691,
                                        504,
                                        722
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                691,
                                                504,
                                                722
                                            ],
                                            "type": "text",
                                            "content": "[85] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, et al. Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 20
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        310,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                310,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        310,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "15"
                                }
                            ]
                        }
                    ],
                    "index": 22
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 14
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        106,
                        72,
                        506,
                        178
                    ],
                    "type": "list",
                    "angle": 0,
                    "index": 4,
                    "blocks": [
                        {
                            "bbox": [
                                106,
                                72,
                                505,
                                93
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        72,
                                        505,
                                        93
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                72,
                                                505,
                                                93
                                            ],
                                            "type": "text",
                                            "content": "[86] P. Zhang, G. Zeng, T. Wang, and W. Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 0
                        },
                        {
                            "bbox": [
                                106,
                                100,
                                506,
                                122
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        100,
                                        506,
                                        122
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                100,
                                                506,
                                                122
                                            ],
                                            "type": "text",
                                            "content": "[87] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022."
                                        }
                                    ]
                                }
                            ],
                            "index": 1
                        },
                        {
                            "bbox": [
                                107,
                                129,
                                504,
                                150
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        129,
                                        504,
                                        150
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                129,
                                                504,
                                                150
                                            ],
                                            "type": "text",
                                            "content": "[88] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv, abs/2304.10592, 2023."
                                        }
                                    ]
                                }
                            ],
                            "index": 2
                        },
                        {
                            "bbox": [
                                106,
                                156,
                                504,
                                178
                            ],
                            "type": "ref_text",
                            "angle": 0,
                            "lines": [
                                {
                                    "bbox": [
                                        106,
                                        156,
                                        504,
                                        178
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                106,
                                                156,
                                                504,
                                                178
                                            ],
                                            "type": "text",
                                            "content": "[89] Z. Zong, B. Ma, D. Shen, G. Song, H. Shao, D. Jiang, H. Li, and Y. Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024."
                                        }
                                    ]
                                }
                            ],
                            "index": 3
                        }
                    ],
                    "sub_type": "ref_text"
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "16"
                                }
                            ]
                        }
                    ],
                    "index": 5
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 15
        },
        {
            "para_blocks": [
                {
                    "bbox": [
                        105,
                        71,
                        212,
                        85
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                71,
                                212,
                                85
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        71,
                                        212,
                                        85
                                    ],
                                    "type": "text",
                                    "content": "A Training Details"
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        105,
                        95,
                        299,
                        108
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                95,
                                299,
                                108
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        95,
                                        299,
                                        108
                                    ],
                                    "type": "text",
                                    "content": "A.1 Custom Supervised Finetuning Dataset"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        104,
                        115,
                        506,
                        171
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                115,
                                506,
                                171
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        115,
                                        506,
                                        171
                                    ],
                                    "type": "text",
                                    "content": "We follow LLaVA-Next [50] to combine a custom data mixture containing 748K SFT data shown in Tab. 9. Following [51, 50], our 748K training data mixture contains (1) LLM instruction following data, e.g. ShareGPT [68]; (2) GPT4/GPT4V generated data, e.g. LLaVA-instruct [51], ShareGPT4V [13], LAION-GPT4V [35]; (3) academic-task-oriented data, e.g. VQAv2 [23], GQA [25], etc."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "type": "table",
                    "bbox": [
                        142,
                        180,
                        470,
                        350
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                142,
                                180,
                                470,
                                350
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        142,
                                        180,
                                        470,
                                        350
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                142,
                                                180,
                                                470,
                                                350
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Dataset</td><td>Size</td><td>Task Prompt</td></tr><tr><td>ShareGPT [68]</td><td>40K</td><td></td></tr><tr><td>LLaVA-instruct [51]</td><td>158K</td><td></td></tr><tr><td>ShareGPT4V [13]</td><td>39K</td><td></td></tr><tr><td>LAION-GPT4V [35]</td><td>11K</td><td></td></tr><tr><td>VQAv2 [23]</td><td>83K</td><td></td></tr><tr><td>GQA [25]</td><td>72K</td><td></td></tr><tr><td>OKVQA [53]</td><td>9K</td><td></td></tr><tr><td>OCRVQA [58]</td><td>80K</td><td></td></tr><tr><td>ChartQA [54]</td><td>7K</td><td>“Answer the question using a single word or phrase.”</td></tr><tr><td>DVQA [29]</td><td>16K</td><td></td></tr><tr><td>DocVQA [56]</td><td>10K</td><td></td></tr><tr><td>AI2D [31]</td><td>2K</td><td></td></tr><tr><td>SynthDog-EN [32]</td><td>20K</td><td></td></tr><tr><td>A-OKVQA [67]</td><td>66K</td><td>“Answer with the option&#x27;s letter from the given choices directly.”</td></tr><tr><td>RefCOCO [30]</td><td>48K</td><td>“Provide a short description for this region.”</td></tr><tr><td>VG [34]</td><td>86K</td><td>“Provide the bounding box coordinate of the region this sentence describes”</td></tr></table>",
                                            "image_path": "7835f23ca7d4cf68673c04af9d195cac10eeb769460392d80553b4f0b6a903d0.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 3,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                212,
                                350,
                                398,
                                360
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        212,
                                        350,
                                        398,
                                        360
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                212,
                                                350,
                                                398,
                                                360
                                            ],
                                            "type": "text",
                                            "content": "Table 9: Data combination of our 748K SFT data."
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        105,
                        387,
                        270,
                        400
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                387,
                                270,
                                400
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        387,
                                        270,
                                        400
                                    ],
                                    "type": "text",
                                    "content": "A.2 Detailed Training Configuration"
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        104,
                        407,
                        504,
                        430
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                407,
                                504,
                                430
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        407,
                                        504,
                                        430
                                    ],
                                    "type": "text",
                                    "content": "We list the detailed training hyper-parameters as follows. For evaluation, we utilize LLMs-Eval [41] for evaluation on several benchmarks."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "type": "table",
                    "bbox": [
                        156,
                        437,
                        455,
                        514
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                156,
                                437,
                                455,
                                514
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        156,
                                        437,
                                        455,
                                        514
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                156,
                                                437,
                                                455,
                                                514
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Hypter-param</td><td>PT</td><td>DeepStack SFT</td><td>DeepStack-V SFT</td><td>DeepStack-HD SFT</td></tr><tr><td>global batch size</td><td>256</td><td>128</td><td>128</td><td>128</td></tr><tr><td>lr</td><td>1e-3</td><td>2e-5</td><td>2e-5</td><td>2e-5</td></tr><tr><td>backbone lr</td><td>freeze</td><td>freeze</td><td>2e-6</td><td>2e-6</td></tr><tr><td>lr schedule</td><td>cosine decay</td><td>cosine decay</td><td>cosine decay</td><td>cosine decay</td></tr><tr><td>lr warmup ratio</td><td>0.03</td><td>0.03</td><td>0.03</td><td>0.03</td></tr><tr><td>epoch</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>optimizer</td><td>AdamW</td><td>AdamW</td><td>AdamW</td><td>AdamW</td></tr></table>",
                                            "image_path": "19f6df30660cbd49f972d949554d6cf2e339ec0b23a4ad882c8af731a52ebced.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 7,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                234,
                                514,
                                374,
                                525
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        234,
                                        514,
                                        374,
                                        525
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                234,
                                                514,
                                                374,
                                                525
                                            ],
                                            "type": "text",
                                            "content": "Table 10: Training hyper-parameters."
                                        }
                                    ]
                                }
                            ],
                            "index": 8,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        105,
                        556,
                        224,
                        571
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                556,
                                224,
                                571
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        556,
                                        224,
                                        571
                                    ],
                                    "type": "text",
                                    "content": "B More Experiments"
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        105,
                        581,
                        366,
                        594
                    ],
                    "type": "title",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                105,
                                581,
                                366,
                                594
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        105,
                                        581,
                                        366,
                                        594
                                    ],
                                    "type": "text",
                                    "content": "B.1 Improved DeepStack-L with Fintuning Vision Encoder"
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        104,
                        601,
                        506,
                        625
                    ],
                    "type": "text",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                104,
                                601,
                                506,
                                625
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        104,
                                        601,
                                        506,
                                        625
                                    ],
                                    "type": "text",
                                    "content": "As shown in Tab. 11, after finetuning the vision encoder, our DeepStack-L achieves further improvement. This further demonstrates the effectiveness and the potential of our DeepStack strategy."
                                }
                            ]
                        }
                    ],
                    "index": 11
                },
                {
                    "type": "table",
                    "bbox": [
                        107,
                        633,
                        504,
                        696
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                107,
                                633,
                                504,
                                696
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        107,
                                        633,
                                        504,
                                        696
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                107,
                                                633,
                                                504,
                                                696
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">LLM</td><td rowspan=\"2\">Eff. Res.</td><td rowspan=\"2\">Vis. Tok.</td><td rowspan=\"2\">Cxt. Len.</td><td rowspan=\"2\">PT</td><td rowspan=\"2\">SFT</td><td rowspan=\"2\">General VQA VQA\\( ^{v2} \\)</td><td rowspan=\"2\">GQA</td><td colspan=\"4\">Text-oriented VQA</td><td colspan=\"4\">LMM benchmarks</td></tr><tr><td>Text VQA\\( ^{\\ddagger} \\)</td><td>Doc VQA\\( ^{\\ddagger} \\)</td><td>Info VQA\\( ^{\\ddagger} \\)</td><td>SEED (all)</td><td>POPE (all)</td><td>MM MU\\( ^{\\ddagger} \\)</td><td>MM Vet</td><td></td></tr><tr><td>DeepStack-L</td><td>Vicuna-7B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>79.5*</td><td>63.1*</td><td>62.4</td><td>39.1</td><td>29.8</td><td>60.6</td><td>86.7</td><td>35.7</td><td>29.9</td><td></td></tr><tr><td>DeepStack-L★</td><td>Vicuna-7B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>81.1*</td><td>63.9*</td><td>64.5</td><td>39.3</td><td>30.1</td><td>63.3</td><td>86.7</td><td>37.1</td><td>29.8</td><td></td></tr><tr><td>DeepStack-L</td><td>Vicuna-13B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>80.9*</td><td>64.2*</td><td>64.6</td><td>41.5</td><td>33.0</td><td>63.5</td><td>87.7</td><td>35.2</td><td>35.9</td><td></td></tr><tr><td>DeepStack-L★</td><td>Vicuna-13B</td><td>672</td><td>2880</td><td>576</td><td>558K</td><td>665K</td><td>82.1*</td><td>65.1*</td><td>65.2</td><td>43.1</td><td>34.0</td><td>64.4</td><td>86.6</td><td>34.7</td><td>36.2</td><td></td></tr></table>",
                                            "image_path": "ee1fc8aa5963e18725ed1c575c4e6f3705c2037e52c1af60634127594be405b5.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 12,
                            "angle": 0,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                186,
                                696,
                                423,
                                706
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        186,
                                        696,
                                        423,
                                        706
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                186,
                                                696,
                                                423,
                                                706
                                            ],
                                            "type": "text",
                                            "content": "Table 11: Improved DeepStack-L with fintuning vision encoder."
                                        }
                                    ]
                                }
                            ],
                            "index": 13,
                            "angle": 0,
                            "type": "table_caption"
                        }
                    ],
                    "index": 12
                }
            ],
            "discarded_blocks": [
                {
                    "bbox": [
                        300,
                        741,
                        311,
                        750
                    ],
                    "type": "page_number",
                    "angle": 0,
                    "lines": [
                        {
                            "bbox": [
                                300,
                                741,
                                311,
                                750
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        300,
                                        741,
                                        311,
                                        750
                                    ],
                                    "type": "text",
                                    "content": "17"
                                }
                            ]
                        }
                    ],
                    "index": 14
                }
            ],
            "page_size": [
                612,
                792
            ],
            "page_idx": 16
        }
    ],
    "_backend": "vlm",
    "_version_name": "2.5.3"
}